{"cells":[{"cell_type":"markdown","id":"2e290fe6","metadata":{"id":"2e290fe6"},"source":["## Global settings and imports"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDqcjKZxkMf3","executionInfo":{"status":"ok","timestamp":1668439614505,"user_tz":-540,"elapsed":3948,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"a645f60e-5341-414d-b86d-cda2e49ebdce"},"id":"fDqcjKZxkMf3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"id":"268a881c","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"268a881c","executionInfo":{"status":"ok","timestamp":1668439617105,"user_tz":-540,"elapsed":2607,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"bc15ef90-72e9-4e33-c56a-b3a008f85c9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["System version: 3.7.15 (default, Oct 12 2022, 19:14:55) \n","[GCC 7.5.0]\n","Tensorflow version: 2.9.2\n"]}],"source":["import sys\n","import os\n","import abc\n","import time\n","import random\n","import re\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","from tensorflow.compat.v1 import keras\n","from tensorflow.compat.v1.linalg import einsum\n","from tensorflow.compat.v1.keras import layers\n","from tensorflow.compat.v1.keras import backend as K\n","\n","\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from sklearn.metrics import (\n","    roc_auc_score,\n","    log_loss,\n","    mean_squared_error,\n","    accuracy_score,\n","    f1_score,\n",")\n","\n","import yaml\n","import zipfile\n","import pickle as pkl\n","\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"markdown","id":"fa80a9a4","metadata":{"id":"fa80a9a4"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"id":"3776c054","metadata":{"id":"3776c054"},"outputs":[],"source":["def flat_config(config):\n","    \"\"\"Flat config loaded from a yaml file to a flat dict.\n","    Args:\n","        config (dict): Configuration loaded from a yaml file.\n","    Returns:\n","        dict: Configuration dictionary.\n","    \"\"\"\n","    f_config = {}\n","    category = config.keys()\n","    for cate in category:\n","        for key, val in config[cate].items():\n","            f_config[key] = val\n","    return f_config"]},{"cell_type":"code","execution_count":null,"id":"a579115a","metadata":{"id":"a579115a"},"outputs":[],"source":["def check_type(config):\n","    \"\"\"Check that the config parameters are the correct type\n","    Args:\n","        config (dict): Configuration dictionary.\n","    Raises:\n","        TypeError: If the parameters are not the correct type.\n","    \"\"\"\n","\n","    int_parameters = [\n","        \"word_size\",\n","        \"his_size\",\n","        \"title_size\",\n","        \"body_size\",\n","        \"npratio\",\n","        \"word_emb_dim\",\n","        \"attention_hidden_dim\",\n","        \"epochs\",\n","        \"batch_size\",\n","        \"show_step\",\n","        \"save_epoch\",\n","        \"head_num\",\n","        \"head_dim\",\n","        \"user_num\",\n","        \"filter_num\",\n","        \"window_size\",\n","        \"gru_unit\",\n","        \"user_emb_dim\",\n","        \"vert_emb_dim\",\n","        \"subvert_emb_dim\",\n","    ]\n","    for param in int_parameters:\n","        if param in config and not isinstance(config[param], int):\n","            raise TypeError(\"Parameters {0} must be int\".format(param))\n","\n","    float_parameters = [\"learning_rate\", \"dropout\"]\n","    for param in float_parameters:\n","        if param in config and not isinstance(config[param], float):\n","            raise TypeError(\"Parameters {0} must be float\".format(param))\n","\n","    str_parameters = [\n","        \"wordEmb_file\",\n","        \"wordDict_file\",\n","        \"userDict_file\",\n","        \"vertDict_file\",\n","        \"subvertDict_file\",\n","        \"method\",\n","        \"loss\",\n","        \"optimizer\",\n","        \"cnn_activation\",\n","        \"dense_activation\" \"type\",\n","    ]\n","    for param in str_parameters:\n","        if param in config and not isinstance(config[param], str):\n","            raise TypeError(\"Parameters {0} must be str\".format(param))\n","\n","    list_parameters = [\"layer_sizes\", \"activation\"]\n","    for param in list_parameters:\n","        if param in config and not isinstance(config[param], list):\n","            raise TypeError(\"Parameters {0} must be list\".format(param))\n","\n","    bool_parameters = [\"support_quick_scoring\"]\n","    for param in bool_parameters:\n","        if param in config and not isinstance(config[param], bool):\n","            raise TypeError(\"Parameters {0} must be bool\".format(param))"]},{"cell_type":"code","execution_count":null,"id":"c0e4e7c7","metadata":{"id":"c0e4e7c7"},"outputs":[],"source":["def check_nn_config(f_config):\n","    \"\"\"Check neural networks configuration.\n","    Args:\n","        f_config (dict): Neural network configuration.\n","    Raises:\n","        ValueError: If the parameters are not correct.\n","    \"\"\"\n","\n","    if f_config[\"model_type\"] in [\"nrms\", \"NRMS\"]:\n","        required_parameters = [\n","            \"title_size\",\n","            \"his_size\",\n","            \"wordEmb_file\",\n","            \"wordDict_file\",\n","            \"userDict_file\",\n","            \"npratio\",\n","            \"data_format\",\n","            \"word_emb_dim\",\n","            # nrms\n","            \"head_num\",\n","            \"head_dim\",\n","            # attention\n","            \"attention_hidden_dim\",\n","            \"loss\",\n","            \"data_format\",\n","            \"dropout\",\n","        ]\n","\n","    elif f_config[\"model_type\"] in [\"naml\", \"NAML\"]:\n","        required_parameters = [\n","            \"title_size\",\n","            \"body_size\",\n","            \"his_size\",\n","            \"wordEmb_file\",\n","            \"subvertDict_file\",\n","            \"vertDict_file\",\n","            \"wordDict_file\",\n","            \"userDict_file\",\n","            \"npratio\",\n","            \"data_format\",\n","            \"word_emb_dim\",\n","            \"vert_emb_dim\",\n","            \"subvert_emb_dim\",\n","            # naml\n","            \"filter_num\",\n","            \"cnn_activation\",\n","            \"window_size\",\n","            \"dense_activation\",\n","            # attention\n","            \"attention_hidden_dim\",\n","            \"loss\",\n","            \"data_format\",\n","            \"dropout\",\n","        ]\n","    elif f_config[\"model_type\"] in [\"lstur\", \"LSTUR\"]:\n","        required_parameters = [\n","            \"title_size\",\n","            \"his_size\",\n","            \"wordEmb_file\",\n","            \"wordDict_file\",\n","            \"userDict_file\",\n","            \"npratio\",\n","            \"data_format\",\n","            \"word_emb_dim\",\n","            # lstur\n","            \"gru_unit\",\n","            \"type\",\n","            \"filter_num\",\n","            \"cnn_activation\",\n","            \"window_size\",\n","            # attention\n","            \"attention_hidden_dim\",\n","            \"loss\",\n","            \"data_format\",\n","            \"dropout\",\n","        ]\n","    elif f_config[\"model_type\"] in [\"npa\", \"NPA\"]:\n","        required_parameters = [\n","            \"title_size\",\n","            \"his_size\",\n","            \"wordEmb_file\",\n","            \"wordDict_file\",\n","            \"userDict_file\",\n","            \"npratio\",\n","            \"data_format\",\n","            \"word_emb_dim\",\n","            # npa\n","            \"user_emb_dim\",\n","            \"filter_num\",\n","            \"cnn_activation\",\n","            \"window_size\",\n","            # attention\n","            \"attention_hidden_dim\",\n","            \"loss\",\n","            \"data_format\",\n","            \"dropout\",\n","        ]\n","    else:\n","        required_parameters = []\n","\n","    # check required parameters\n","    for param in required_parameters:\n","        if param not in f_config:\n","            raise ValueError(\"Parameters {0} must be set\".format(param))\n","\n","    if f_config[\"model_type\"] in [\"nrms\", \"NRMS\", \"lstur\", \"LSTUR\"]:\n","        if f_config[\"data_format\"] != \"news\":\n","            raise ValueError(\n","                \"For nrms and naml model, data format must be 'news', but your set is {0}\".format(\n","                    f_config[\"data_format\"]\n","                )\n","            )\n","    elif f_config[\"model_type\"] in [\"naml\", \"NAML\"]:\n","        if f_config[\"data_format\"] != \"naml\":\n","            raise ValueError(\n","                \"For nrms and naml model, data format must be 'naml', but your set is {0}\".format(\n","                    f_config[\"data_format\"]\n","                )\n","            )\n","\n","    check_type(f_config)"]},{"cell_type":"code","execution_count":null,"id":"0c48b20d","metadata":{"id":"0c48b20d"},"outputs":[],"source":["def load_yaml(filename):\n","    \"\"\"Load a yaml file.\n","    Args:\n","        filename (str): Filename.\n","    Returns:\n","        dict: Dictionary.\n","    \"\"\"\n","    try:\n","        with open(filename, \"r\") as f:\n","            config = yaml.load(f, yaml.SafeLoader)\n","        return config\n","    except FileNotFoundError:  # for file not found\n","        raise\n","    except Exception:  # for other exceptions\n","        raise IOError(\"load {0} error!\".format(filename))"]},{"cell_type":"code","execution_count":null,"id":"428d0e9b","metadata":{"id":"428d0e9b"},"outputs":[],"source":["class HParams:\n","    \"\"\"Class for holding hyperparameters for DeepRec algorithms.\"\"\"\n","\n","    def __init__(self, hparams_dict):\n","        \"\"\"Create an HParams object from a dictionary of hyperparameter values.\n","        Args:\n","            hparams_dict (dict): Dictionary with the model hyperparameters.\n","        \"\"\"\n","        for val in hparams_dict.values():\n","            if not (\n","                isinstance(val, int)\n","                or isinstance(val, float)\n","                or isinstance(val, str)\n","                or isinstance(val, list)\n","            ):\n","                raise ValueError(\n","                    \"Hyperparameter value {} should be integer, float, string or list.\".format(\n","                        val\n","                    )\n","                )\n","        self._values = hparams_dict\n","        for hparam in hparams_dict:\n","            setattr(self, hparam, hparams_dict[hparam])\n","\n","    def __repr__(self):\n","        return \"HParams object with values {}\".format(self._values.__repr__())\n","\n","    def values(self):\n","        \"\"\"Return the hyperparameter values as a dictionary.\n","        Returns:\n","            dict: Dictionary with the hyperparameter values.\n","        \"\"\"\n","        return self._values"]},{"cell_type":"code","execution_count":null,"id":"0dd9e5ac","metadata":{"id":"0dd9e5ac"},"outputs":[],"source":["def create_hparams(flags):\n","    \"\"\"Create the model hyperparameters.\n","    Args:\n","        flags (dict): Dictionary with the model requirements.\n","    Returns:\n","        HParams: Hyperparameter object.\n","    \"\"\"\n","    init_dict = {\n","        # data\n","        \"support_quick_scoring\": False,\n","        # models\n","        \"dropout\": 0.0,\n","        \"attention_hidden_dim\": 200,\n","        # nrms\n","        \"head_num\": 4,\n","        \"head_dim\": 100,\n","        # naml\n","        \"filter_num\": 200,\n","        \"window_size\": 3,\n","        \"vert_emb_dim\": 100,\n","        \"subvert_emb_dim\": 100,\n","        # lstur\n","        \"gru_unit\": 400,\n","        \"type\": \"ini\",\n","        # npa\n","        \"user_emb_dim\": 50,\n","        # train\n","        \"learning_rate\": 0.001,\n","        \"optimizer\": \"adam\",\n","        \"epochs\": 10,\n","        \"batch_size\": 1,\n","        # show info\n","        \"show_step\": 1,\n","    }\n","    init_dict.update(flags)\n","    return HParams(init_dict)"]},{"cell_type":"code","execution_count":null,"id":"bb97481c","metadata":{"id":"bb97481c"},"outputs":[],"source":["def prepare_hparams(yaml_file=None, **kwargs):\n","    \"\"\"Prepare the model hyperparameters and check that all have the correct value.\n","    Args:\n","        yaml_file (str): YAML file as configuration.\n","    Returns:\n","        HParams: Hyperparameter object.\n","    \"\"\"\n","    if yaml_file is not None:\n","        config = load_yaml(yaml_file)\n","        config = flat_config(config)\n","    else:\n","        config = {}\n","\n","    config.update(kwargs)\n","\n","    check_nn_config(config)\n","    return create_hparams(config)"]},{"cell_type":"code","execution_count":null,"id":"c884fcae","metadata":{"id":"c884fcae"},"outputs":[],"source":["def word_tokenize(sent):\n","    \"\"\"Split sentence into word list using regex.\n","    Args:\n","        sent (str): Input sentence\n","    Return:\n","        list: word list\n","    \"\"\"\n","    pat = re.compile(r\"[\\w]+|[.,!?;|]\")\n","    if isinstance(sent, str):\n","        return pat.findall(sent.lower())\n","    else:\n","        return []"]},{"cell_type":"code","execution_count":null,"id":"0377893a","metadata":{"id":"0377893a"},"outputs":[],"source":["def newsample(news, ratio):\n","    \"\"\"Sample ratio samples from news list.\n","    If length of news is less than ratio, pad zeros.\n","    Args:\n","        news (list): input news list\n","        ratio (int): sample number\n","    Returns:\n","        list: output of sample list.\n","    \"\"\"\n","    if ratio > len(news):\n","        return news + [0] * (ratio - len(news))\n","    else:\n","        return random.sample(news, ratio)"]},{"cell_type":"code","execution_count":null,"id":"92d3dd0d","metadata":{"id":"92d3dd0d"},"outputs":[],"source":["def mrr_score(y_true, y_score):\n","    \"\"\"Computing mrr score metric.\n","    Args:\n","        y_true (np.ndarray): Ground-truth labels.\n","        y_score (np.ndarray): Predicted labels.\n","    Returns:\n","        numpy.ndarray: mrr scores.\n","    \"\"\"\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order)\n","    rr_score = y_true / (np.arange(len(y_true)) + 1)\n","    return np.sum(rr_score) / np.sum(y_true)\n","\n","\n","def ndcg_score(y_true, y_score, k=10):\n","    \"\"\"Computing ndcg score metric at k.\n","    Args:\n","        y_true (np.ndarray): Ground-truth labels.\n","        y_score (np.ndarray): Predicted labels.\n","    Returns:\n","        numpy.ndarray: ndcg scores.\n","    \"\"\"\n","    best = dcg_score(y_true, y_true, k)\n","    actual = dcg_score(y_true, y_score, k)\n","    return actual / best\n","\n","\n","def hit_score(y_true, y_score, k=10):\n","    \"\"\"Computing hit score metric at k.\n","    Args:\n","        y_true (np.ndarray): ground-truth labels.\n","        y_score (np.ndarray): predicted labels.\n","    Returns:\n","        np.ndarray: hit score.\n","    \"\"\"\n","    ground_truth = np.where(y_true == 1)[0]\n","    argsort = np.argsort(y_score)[::-1][:k]\n","    for idx in argsort:\n","        if idx in ground_truth:\n","            return 1\n","    return 0\n","\n","\n","def dcg_score(y_true, y_score, k=10):\n","    \"\"\"Computing dcg score metric at k.\n","    Args:\n","        y_true (np.ndarray): Ground-truth labels.\n","        y_score (np.ndarray): Predicted labels.\n","    Returns:\n","        np.ndarray: dcg scores.\n","    \"\"\"\n","    k = min(np.shape(y_true)[-1], k)\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order[:k])\n","    gains = 2 ** y_true - 1\n","    discounts = np.log2(np.arange(len(y_true)) + 2)\n","    return np.sum(gains / discounts)"]},{"cell_type":"code","execution_count":null,"id":"92587640","metadata":{"id":"92587640"},"outputs":[],"source":["def cal_metric(labels, preds, metrics):\n","    \"\"\"Calculate metrics.\n","    Available options are: `auc`, `rmse`, `logloss`, `acc` (accurary), `f1`, `mean_mrr`,\n","    `ndcg` (format like: ndcg@2;4;6;8), `hit` (format like: hit@2;4;6;8), `group_auc`.\n","    Args:\n","        labels (array-like): Labels.\n","        preds (array-like): Predictions.\n","        metrics (list): List of metric names.\n","    Return:\n","        dict: Metrics.\n","    Examples:\n","        >>> cal_metric(labels, preds, [\"ndcg@2;4;6\", \"group_auc\"])\n","        {'ndcg@2': 0.4026, 'ndcg@4': 0.4953, 'ndcg@6': 0.5346, 'group_auc': 0.8096}\n","    \"\"\"\n","    res = {}\n","    for metric in metrics:\n","        if metric == \"auc\":\n","            auc = roc_auc_score(np.asarray(labels), np.asarray(preds))\n","            res[\"auc\"] = round(auc, 4)\n","        elif metric == \"rmse\":\n","            rmse = mean_squared_error(np.asarray(labels), np.asarray(preds))\n","            res[\"rmse\"] = np.sqrt(round(rmse, 4))\n","        elif metric == \"logloss\":\n","            # avoid logloss nan\n","            preds = [max(min(p, 1.0 - 10e-12), 10e-12) for p in preds]\n","            logloss = log_loss(np.asarray(labels), np.asarray(preds))\n","            res[\"logloss\"] = round(logloss, 4)\n","        elif metric == \"acc\":\n","            pred = np.asarray(preds)\n","            pred[pred >= 0.5] = 1\n","            pred[pred < 0.5] = 0\n","            acc = accuracy_score(np.asarray(labels), pred)\n","            res[\"acc\"] = round(acc, 4)\n","        elif metric == \"f1\":\n","            pred = np.asarray(preds)\n","            pred[pred >= 0.5] = 1\n","            pred[pred < 0.5] = 0\n","            f1 = f1_score(np.asarray(labels), pred)\n","            res[\"f1\"] = round(f1, 4)\n","        elif metric == \"mean_mrr\":\n","            mean_mrr = np.mean(\n","                [\n","                    mrr_score(each_labels, each_preds)\n","                    for each_labels, each_preds in zip(labels, preds)\n","                ]\n","            )\n","            res[\"mean_mrr\"] = round(mean_mrr, 4)\n","        elif metric.startswith(\"ndcg\"):  # format like:  ndcg@2;4;6;8\n","            ndcg_list = [1, 2]\n","            ks = metric.split(\"@\")\n","            if len(ks) > 1:\n","                ndcg_list = [int(token) for token in ks[1].split(\";\")]\n","            for k in ndcg_list:\n","                ndcg_temp = np.mean(\n","                    [\n","                        ndcg_score(each_labels, each_preds, k)\n","                        for each_labels, each_preds in zip(labels, preds)\n","                    ]\n","                )\n","                res[\"ndcg@{0}\".format(k)] = round(ndcg_temp, 4)\n","        elif metric.startswith(\"hit\"):  # format like:  hit@2;4;6;8\n","            hit_list = [1, 2]\n","            ks = metric.split(\"@\")\n","            if len(ks) > 1:\n","                hit_list = [int(token) for token in ks[1].split(\";\")]\n","            for k in hit_list:\n","                hit_temp = np.mean(\n","                    [\n","                        hit_score(each_labels, each_preds, k)\n","                        for each_labels, each_preds in zip(labels, preds)\n","                    ]\n","                )\n","                res[\"hit@{0}\".format(k)] = round(hit_temp, 4)\n","        elif metric == \"group_auc\":\n","            group_auc = np.mean(\n","                [\n","                    roc_auc_score(each_labels, each_preds)\n","                    for each_labels, each_preds in zip(labels, preds)\n","                ]\n","            )\n","            res[\"group_auc\"] = round(group_auc, 4)\n","        else:\n","            raise ValueError(\"Metric {0} not defined\".format(metric))\n","    return res"]},{"cell_type":"markdown","id":"1737ccf0","metadata":{"id":"1737ccf0"},"source":["## Prepare parameters"]},{"cell_type":"code","execution_count":null,"id":"8d5e7d89","metadata":{"tags":["parameters"],"id":"8d5e7d89"},"outputs":[],"source":["epochs = 10\n","seed = 42\n","batch_size = 32\n","\n","data_path = '/content/drive/MyDrive/GH x RippleAI/Dataset/movielens/MINDLike'"]},{"cell_type":"markdown","id":"b63a3b4e","metadata":{"id":"b63a3b4e"},"source":["## Load and Split Data"]},{"cell_type":"markdown","id":"96315ab5","metadata":{"id":"96315ab5"},"source":["### Train / Valid Split"]},{"cell_type":"code","execution_count":null,"id":"1bd6242b","metadata":{"id":"1bd6242b"},"outputs":[],"source":["items_file = os.path.join(data_path,'items.tsv')\n","behaviors_file = os.path.join(data_path,'behaviors.tsv')"]},{"cell_type":"code","execution_count":null,"id":"906b84df","metadata":{"id":"906b84df"},"outputs":[],"source":["def train_valid_split(items_file, behaviors_file):\n","    items_df = pd.read_csv(items_file,sep='\\t',header=None)\n","    behav_df = pd.read_csv(behaviors_file,sep='\\t',header=None)\n","    \n","    \n","    def extract_items(x):\n","        items_list = x.split(' ')\n","        for i in range(len(items_list)):\n","            items_list[i] = items_list[i].split('-')[0]\n","        return items_list\n","    \n","    behav_df[4] = behav_df[3].apply(lambda x: extract_items(x))\n","    \n","    train_behav = behav_df.set_index(0).loc[:behav_df[0].nunique()*0.8].reset_index()\n","    valid_behav = behav_df.set_index(0).loc[behav_df[0].nunique()*0.8+1:].reset_index()\n","    \n","    \n","    if not os.path.exists(os.path.join(data_path,'train')):\n","        os.mkdir(os.path.join(data_path,'train'))\n","    \n","    if not os.path.exists(os.path.join(data_path,'valid')):\n","        os.mkdir(os.path.join(data_path,'valid'))\n","    \n","    \n","    train_items_set, valid_items_set = set(), set()\n","    \n","\n","    if not os.path.exists(os.path.join(data_path,'train', r'items.tsv')):\n","        for i in train_behav[2].dropna().apply(lambda x: x.split(' ')):\n","            train_items_set.update(set(i))\n","\n","        for i in train_behav[4]:\n","            train_items_set.update(set(i))\n","\n","        items_df[4] = items_df[0].astype(str).apply(lambda x: x in train_items_set)\n","        items_df.drop(4,axis=1).to_csv(os.path.join(data_path,'train', r'items.tsv'),sep='\\t',index=False, header=False)\n","    \n","    \n","    if not os.path.exists(os.path.join(data_path,'valid', r'items.tsv')):\n","        for i in valid_behav[2].dropna().apply(lambda x: x.split(' ')):\n","            valid_items_set.update(set(i))\n","\n","        for i in valid_behav[4]:\n","            valid_items_set.update(set(i))\n","\n","        items_df[4] = items_df[0].astype(str).apply(lambda x: x in valid_items_set)\n","        items_df.drop(4,axis=1).to_csv(os.path.join(data_path,'valid', r'items.tsv'),sep='\\t',index=False, header=False)\n","    \n","    \n","    if not os.path.exists(os.path.join(data_path,'train', r'behaviors.tsv')):    \n","        train_behav.drop(4,axis=1).to_csv(os.path.join(data_path,'train', r'behaviors.tsv'),sep='\\t',index=False, header=False)\n","        \n","    if not os.path.exists(os.path.join(data_path,'valid', r'behaviors.tsv')):   \n","        valid_behav.drop(4,axis=1).to_csv(os.path.join(data_path,'valid', r'behaviors.tsv'),sep='\\t',index=False, header=False)"]},{"cell_type":"code","execution_count":null,"id":"8d1b26e4","metadata":{"id":"8d1b26e4"},"outputs":[],"source":["train_valid_split(items_file, behaviors_file)"]},{"cell_type":"markdown","id":"caf8ae18","metadata":{"id":"caf8ae18"},"source":["### Load Data"]},{"cell_type":"code","execution_count":null,"id":"46733aef","metadata":{"id":"46733aef"},"outputs":[],"source":["train_items_file = os.path.join(data_path, 'train', r'items.tsv')\n","train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n","valid_items_file = os.path.join(data_path, 'valid', r'items.tsv')\n","valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n","wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n","userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n","wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n","yaml_file = os.path.join(data_path, \"utils\", r'nrms.yaml')"]},{"cell_type":"markdown","id":"dc0129f0","metadata":{"id":"dc0129f0"},"source":["## Create hyper-parameters"]},{"cell_type":"code","execution_count":null,"id":"21e8ef18","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"21e8ef18","executionInfo":{"status":"ok","timestamp":1668439624962,"user_tz":-540,"elapsed":36,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"d13b0363-2d5d-488c-f9cc-e35f306de581"},"outputs":[{"output_type":"stream","name":"stdout","text":["HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 20, 'head_dim': 20, 'filter_num': 200, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 10, 'batch_size': 32, 'show_step': 10, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'model_type': 'nrms', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/content/drive/MyDrive/GH x RippleAI/Dataset/movielens/MINDLike/utils/embedding.npy', 'wordDict_file': '/content/drive/MyDrive/GH x RippleAI/Dataset/movielens/MINDLike/utils/word_dict.pkl', 'userDict_file': '/content/drive/MyDrive/GH x RippleAI/Dataset/movielens/MINDLike/utils/uid2index.pkl'}\n"]}],"source":["hparams = prepare_hparams(yaml_file, \n","                          wordEmb_file=wordEmb_file,\n","                          wordDict_file=wordDict_file, \n","                          userDict_file=userDict_file,\n","                          batch_size=batch_size,\n","                          epochs=epochs,\n","                          show_step=10)\n","print(hparams)"]},{"cell_type":"markdown","id":"1f74f656","metadata":{"id":"1f74f656"},"source":["## Model 정의"]},{"cell_type":"markdown","id":"d5447178","metadata":{"id":"d5447178"},"source":["### Base Model"]},{"cell_type":"code","execution_count":null,"id":"279c8c0c","metadata":{"id":"279c8c0c"},"outputs":[],"source":["tf.compat.v1.disable_eager_execution()\n","tf.compat.v1.experimental.output_all_intermediates(True)\n","\n","class BaseModel:\n","    \"\"\"Basic class of models\n","    Attributes:\n","        hparams (HParams): A HParams object, holds the entire set of hyperparameters.\n","        train_iterator (object): An iterator to load the data in training steps.\n","        test_iterator (object): An iterator to load the data in testing steps.\n","        graph (object): An optional graph.\n","        seed (int): Random seed.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        hparams,\n","        iterator_creator,\n","        seed=None,\n","    ):\n","        \"\"\"Initializing the model. Create common logics which are needed by all deeprec models, such as loss function,\n","        parameter set.\n","        Args:\n","            hparams (HParams): A HParams object, holds the entire set of hyperparameters.\n","            iterator_creator (object): An iterator to load the data.\n","            graph (object): An optional graph.\n","            seed (int): Random seed.\n","        \"\"\"\n","        self.seed = seed\n","        tf.compat.v1.set_random_seed(seed)\n","        np.random.seed(seed)\n","\n","        self.train_iterator = iterator_creator(\n","            hparams,\n","            hparams.npratio,\n","            col_spliter=\"\\t\",\n","        )\n","        self.test_iterator = iterator_creator(\n","            hparams,\n","            col_spliter=\"\\t\",\n","        )\n","\n","        self.hparams = hparams\n","        self.support_quick_scoring = hparams.support_quick_scoring\n","\n","        # set GPU use with on demand growth\n","        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n","        sess = tf.compat.v1.Session(\n","            config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n","        )\n","\n","        # set this TensorFlow session as the default session for Keras\n","        tf.compat.v1.keras.backend.set_session(sess)\n","\n","        # IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras!\n","        # Otherwise, their weights will be unavailable in the threads after the session there has been set\n","        self.model, self.scorer = self._build_graph()\n","\n","        self.loss = self._get_loss()\n","        self.train_optimizer = self._get_opt()\n","\n","        self.model.compile(loss=self.loss, optimizer=self.train_optimizer)\n","\n","    def _init_embedding(self, file_path):\n","        \"\"\"Load pre-trained embeddings as a constant tensor.\n","        Args:\n","            file_path (str): the pre-trained glove embeddings file path.\n","        Returns:\n","            numpy.ndarray: A constant numpy array.\n","        \"\"\"\n","\n","        return np.load(file_path)\n","\n","    @abc.abstractmethod\n","    def _build_graph(self):\n","        \"\"\"Subclass will implement this.\"\"\"\n","        pass\n","\n","    @abc.abstractmethod\n","    def _get_input_label_from_iter(self, batch_data):\n","        \"\"\"Subclass will implement this\"\"\"\n","        pass\n","\n","    def _get_loss(self):\n","        \"\"\"Make loss function, consists of data loss and regularization loss\n","        Returns:\n","            object: Loss function or loss function name\n","        \"\"\"\n","        if self.hparams.loss == \"cross_entropy_loss\":\n","            data_loss = \"categorical_crossentropy\"\n","        elif self.hparams.loss == \"log_loss\":\n","            data_loss = \"binary_crossentropy\"\n","        else:\n","            raise ValueError(\"this loss not defined {0}\".format(self.hparams.loss))\n","        return data_loss\n","\n","    def _get_opt(self):\n","        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n","        Returns:\n","            object: An optimizer.\n","        \"\"\"\n","        lr = self.hparams.learning_rate\n","        optimizer = self.hparams.optimizer\n","\n","        if optimizer == \"adam\":\n","            train_opt = keras.optimizers.Adam(lr=lr)\n","\n","        return train_opt\n","\n","    def _get_pred(self, logit, task):\n","        \"\"\"Make final output as prediction score, according to different tasks.\n","        Args:\n","            logit (object): Base prediction value.\n","            task (str): A task (values: regression/classification)\n","        Returns:\n","            object: Transformed score\n","        \"\"\"\n","        if task == \"regression\":\n","            pred = tf.identity(logit)\n","        elif task == \"classification\":\n","            pred = tf.sigmoid(logit)\n","        else:\n","            raise ValueError(\n","                \"method must be regression or classification, but now is {0}\".format(\n","                    task\n","                )\n","            )\n","        return pred\n","\n","    def train(self, train_batch_data):\n","        \"\"\"Go through the optimization step once with training data in feed_dict.\n","        Args:\n","            sess (object): The model session object.\n","            feed_dict (dict): Feed values to train the model. This is a dictionary that maps graph elements to values.\n","        Returns:\n","            list: A list of values, including update operation, total loss, data loss, and merged summary.\n","        \"\"\"\n","        train_input, train_label = self._get_input_label_from_iter(train_batch_data)\n","        rslt = self.model.train_on_batch(train_input, train_label)\n","        return rslt\n","\n","    def eval(self, eval_batch_data):\n","        \"\"\"Evaluate the data in feed_dict with current model.\n","        Args:\n","            sess (object): The model session object.\n","            feed_dict (dict): Feed values for evaluation. This is a dictionary that maps graph elements to values.\n","        Returns:\n","            list: A list of evaluated results, including total loss value, data loss value, predicted scores, and ground-truth labels.\n","        \"\"\"\n","        eval_input, eval_label = self._get_input_label_from_iter(eval_batch_data)\n","        imp_index = eval_batch_data[\"impression_index_batch\"]\n","\n","        pred_rslt = self.scorer.predict_on_batch(eval_input)\n","\n","        return pred_rslt, eval_label, imp_index\n","\n","    def fit(\n","        self,\n","        train_news_file,\n","        train_behaviors_file,\n","        valid_news_file,\n","        valid_behaviors_file,\n","        test_news_file=None,\n","        test_behaviors_file=None,\n","    ):\n","        \"\"\"Fit the model with train_file. Evaluate the model on valid_file per epoch to observe the training status.\n","        If test_news_file is not None, evaluate it too.\n","        Args:\n","            train_file (str): training data set.\n","            valid_file (str): validation set.\n","            test_news_file (str): test set.\n","        Returns:\n","            object: An instance of self.\n","        \"\"\"\n","\n","        for epoch in range(1, self.hparams.epochs + 1):\n","            step = 0\n","            self.hparams.current_epoch = epoch\n","            epoch_loss = 0\n","            train_start = time.time()\n","\n","            tqdm_util = tqdm(\n","                self.train_iterator.load_data_from_file(\n","                    train_news_file, train_behaviors_file\n","                )\n","            )\n","\n","            for batch_data_input in tqdm_util:\n","\n","                step_result = self.train(batch_data_input)\n","                step_data_loss = step_result\n","\n","                epoch_loss += step_data_loss\n","                step += 1\n","                if step % self.hparams.show_step == 0:\n","                    tqdm_util.set_description(\n","                        \"step {0:d} , total_loss: {1:.4f}, data_loss: {2:.4f}\".format(\n","                            step, epoch_loss / step, step_data_loss\n","                        )\n","                    )\n","\n","            train_end = time.time()\n","            train_time = train_end - train_start\n","\n","            eval_start = time.time()\n","\n","            train_info = \",\".join(\n","                [\n","                    str(item[0]) + \":\" + str(item[1])\n","                    for item in [(\"logloss loss\", epoch_loss / step)]\n","                ]\n","            )\n","\n","            eval_res = self.run_eval(valid_news_file, valid_behaviors_file)\n","            eval_info = \", \".join(\n","                [\n","                    str(item[0]) + \":\" + str(item[1])\n","                    for item in sorted(eval_res.items(), key=lambda x: x[0])\n","                ]\n","            )\n","            if test_news_file is not None:\n","                test_res = self.run_eval(test_news_file, test_behaviors_file)\n","                test_info = \", \".join(\n","                    [\n","                        str(item[0]) + \":\" + str(item[1])\n","                        for item in sorted(test_res.items(), key=lambda x: x[0])\n","                    ]\n","                )\n","            eval_end = time.time()\n","            eval_time = eval_end - eval_start\n","\n","            if test_news_file is not None:\n","                print(\n","                    \"at epoch {0:d}\".format(epoch)\n","                    + \"\\ntrain info: \"\n","                    + train_info\n","                    + \"\\neval info: \"\n","                    + eval_info\n","                    + \"\\ntest info: \"\n","                    + test_info\n","                )\n","            else:\n","                print(\n","                    \"at epoch {0:d}\".format(epoch)\n","                    + \"\\ntrain info: \"\n","                    + train_info\n","                    + \"\\neval info: \"\n","                    + eval_info\n","                )\n","            print(\n","                \"at epoch {0:d} , train time: {1:.1f} eval time: {2:.1f}\".format(\n","                    epoch, train_time, eval_time\n","                )\n","            )\n","\n","        return self\n","\n","    def group_labels(self, labels, preds, group_keys):\n","        \"\"\"Devide labels and preds into several group according to values in group keys.\n","        Args:\n","            labels (list): ground truth label list.\n","            preds (list): prediction score list.\n","            group_keys (list): group key list.\n","        Returns:\n","            list, list, list:\n","            - Keys after group.\n","            - Labels after group.\n","            - Preds after group.\n","        \"\"\"\n","\n","        all_keys = list(set(group_keys))\n","        all_keys.sort()\n","        group_labels = {k: [] for k in all_keys}\n","        group_preds = {k: [] for k in all_keys}\n","\n","        for label, p, k in zip(labels, preds, group_keys):\n","            group_labels[k].append(label)\n","            group_preds[k].append(p)\n","\n","        all_labels = []\n","        all_preds = []\n","        for k in all_keys:\n","            all_labels.append(group_labels[k])\n","            all_preds.append(group_preds[k])\n","\n","        return all_keys, all_labels, all_preds\n","\n","    def run_eval(self, news_filename, behaviors_file):\n","        \"\"\"Evaluate the given file and returns some evaluation metrics.\n","        Args:\n","            filename (str): A file name that will be evaluated.\n","        Returns:\n","            dict: A dictionary that contains evaluation metrics.\n","        \"\"\"\n","\n","        if self.support_quick_scoring:\n","            _, group_labels, group_preds = self.run_fast_eval(\n","                news_filename, behaviors_file\n","            )\n","        else:\n","            _, group_labels, group_preds = self.run_slow_eval(\n","                news_filename, behaviors_file\n","            )\n","        res = cal_metric(group_labels, group_preds, self.hparams.metrics)\n","        return res\n","\n","    def user(self, batch_user_input):\n","        user_input = self._get_user_feature_from_iter(batch_user_input)\n","        user_vec = self.userencoder.predict_on_batch(user_input)\n","        user_index = batch_user_input[\"impr_index_batch\"]\n","\n","        return user_index, user_vec\n","\n","    def news(self, batch_news_input):\n","        news_input = self._get_news_feature_from_iter(batch_news_input)\n","        news_vec = self.newsencoder.predict_on_batch(news_input)\n","        news_index = batch_news_input[\"news_index_batch\"]\n","\n","        return news_index, news_vec\n","\n","    def run_user(self, news_filename, behaviors_file):\n","        if not hasattr(self, \"userencoder\"):\n","            raise ValueError(\"model must have attribute userencoder\")\n","\n","        user_indexes = []\n","        user_vecs = []\n","        for batch_data_input in tqdm(\n","            self.test_iterator.load_user_from_file(news_filename, behaviors_file)\n","        ):\n","            user_index, user_vec = self.user(batch_data_input)\n","            user_indexes.extend(np.reshape(user_index, -1))\n","            user_vecs.extend(user_vec)\n","\n","        return dict(zip(user_indexes, user_vecs))\n","\n","    def run_news(self, news_filename):\n","        if not hasattr(self, \"newsencoder\"):\n","            raise ValueError(\"model must have attribute newsencoder\")\n","\n","        news_indexes = []\n","        news_vecs = []\n","        for batch_data_input in tqdm(\n","            self.test_iterator.load_news_from_file(news_filename)\n","        ):\n","            news_index, news_vec = self.news(batch_data_input)\n","            news_indexes.extend(np.reshape(news_index, -1))\n","            news_vecs.extend(news_vec)\n","\n","        return dict(zip(news_indexes, news_vecs))\n","\n","    def run_slow_eval(self, news_filename, behaviors_file):\n","        preds = []\n","        labels = []\n","        imp_indexes = []\n","\n","        for batch_data_input in tqdm(\n","            self.test_iterator.load_data_from_file(news_filename, behaviors_file)\n","        ):\n","            step_pred, step_labels, step_imp_index = self.eval(batch_data_input)\n","            preds.extend(np.reshape(step_pred, -1))\n","            labels.extend(np.reshape(step_labels, -1))\n","            imp_indexes.extend(np.reshape(step_imp_index, -1))\n","\n","        group_impr_indexes, group_labels, group_preds = self.group_labels(\n","            labels, preds, imp_indexes\n","        )\n","        return group_impr_indexes, group_labels, group_preds\n","\n","    def run_fast_eval(self, news_filename, behaviors_file):\n","        news_vecs = self.run_news(news_filename)\n","        user_vecs = self.run_user(news_filename, behaviors_file)\n","\n","        self.news_vecs = news_vecs\n","        self.user_vecs = user_vecs\n","\n","        group_impr_indexes = []\n","        group_labels = []\n","        group_preds = []\n","\n","        for (\n","            impr_index,\n","            news_index,\n","            user_index,\n","            label,\n","        ) in tqdm(self.test_iterator.load_impression_from_file(behaviors_file)):\n","            pred = np.dot(\n","                np.stack([news_vecs[i] for i in news_index], axis=0),\n","                user_vecs[impr_index],\n","            )\n","            group_impr_indexes.append(impr_index)\n","            group_labels.append(label)\n","            group_preds.append(pred)\n","\n","        return group_impr_indexes, group_labels, group_preds"]},{"cell_type":"markdown","id":"e38c6e8e","metadata":{"id":"e38c6e8e"},"source":["### Layers"]},{"cell_type":"code","execution_count":null,"id":"4b3b0d94","metadata":{"id":"4b3b0d94"},"outputs":[],"source":["class AttLayer2(layers.Layer):\n","    \"\"\"Soft alignment attention implement.\n","    Attributes:\n","        dim (int): attention hidden dim\n","    \"\"\"\n","\n","    def __init__(self, dim=200, seed=0, **kwargs):\n","        \"\"\"Initialization steps for AttLayer2.\n","        Args:\n","            dim (int): attention hidden dim\n","        \"\"\"\n","\n","        self.dim = dim\n","        self.seed = seed\n","        super(AttLayer2, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        \"\"\"Initialization for variables in AttLayer2\n","        There are there variables in AttLayer2, i.e. W, b and q.\n","        Args:\n","            input_shape (object): shape of input tensor.\n","        \"\"\"\n","\n","        assert len(input_shape) == 3\n","        dim = self.dim\n","        self.W = self.add_weight(\n","            name=\"W\",\n","            shape=(int(input_shape[-1]), dim),\n","            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n","            trainable=True,\n","        )\n","        self.b = self.add_weight(\n","            name=\"b\",\n","            shape=(dim,),\n","            initializer=keras.initializers.Zeros(),\n","            trainable=True,\n","        )\n","        self.q = self.add_weight(\n","            name=\"q\",\n","            shape=(dim, 1),\n","            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n","            trainable=True,\n","        )\n","        super(AttLayer2, self).build(input_shape)  # be sure you call this somewhere!\n","\n","    def call(self, inputs, mask=None, **kwargs):\n","        \"\"\"Core implemention of soft attention\n","        Args:\n","            inputs (object): input tensor.\n","        Returns:\n","            object: weighted sum of input tensors.\n","        \"\"\"\n","\n","        attention = K.tanh(K.dot(inputs, self.W) + self.b)\n","        attention = K.dot(attention, self.q)\n","\n","        attention = K.squeeze(attention, axis=2)\n","\n","        if mask is None:\n","            attention = K.exp(attention)\n","        else:\n","            attention = K.exp(attention) * K.cast(mask, dtype=\"float32\")\n","\n","        attention_weight = attention / (\n","            K.sum(attention, axis=-1, keepdims=True) + K.epsilon()\n","        )\n","\n","        attention_weight = K.expand_dims(attention_weight)\n","        weighted_input = inputs * attention_weight\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        \"\"\"Compte output mask value\n","        Args:\n","            input (object): input tensor.\n","            input_mask: input mask\n","        Returns:\n","            object: output mask.\n","        \"\"\"\n","        return None\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"Compute shape of output tensor\n","        Args:\n","            input_shape (tuple): shape of input tensor.\n","        Returns:\n","            tuple: shape of output tensor.\n","        \"\"\"\n","        return input_shape[0], input_shape[-1]"]},{"cell_type":"code","source":["class SelfAttention(layers.Layer):\n","    \"\"\"Multi-head self attention implement.\n","    Args:\n","        multiheads (int): The number of heads.\n","        head_dim (object): Dimention of each head.\n","        mask_right (boolean): whether to mask right words.\n","    Returns:\n","        object: Weighted sum after attention.\n","    \"\"\"\n","\n","    def __init__(self, multiheads, head_dim, seed=0, mask_right=False, **kwargs):\n","        \"\"\"Initialization steps for AttLayer2.\n","        Args:\n","            multiheads (int): The number of heads.\n","            head_dim (object): Dimension of each head.\n","            mask_right (boolean): Whether to mask right words.\n","        \"\"\"\n","\n","        self.multiheads = multiheads\n","        self.head_dim = head_dim\n","        self.output_dim = multiheads * head_dim\n","        self.mask_right = mask_right\n","        self.seed = seed\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"Compute shape of output tensor.\n","        Returns:\n","            tuple: output shape tuple.\n","        \"\"\"\n","\n","        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n","\n","    def build(self, input_shape):\n","        \"\"\"Initialization for variables in SelfAttention.\n","        There are three variables in SelfAttention, i.e. WQ, WK ans WV.\n","        WQ is used for linear transformation of query.\n","        WK is used for linear transformation of key.\n","        WV is used for linear transformation of value.\n","        Args:\n","            input_shape (object): shape of input tensor.\n","        \"\"\"\n","\n","        self.WQ = self.add_weight(\n","            name=\"WQ\",\n","            shape=(int(input_shape[0][-1]), self.output_dim),\n","            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n","            trainable=True,\n","        )\n","        self.WK = self.add_weight(\n","            name=\"WK\",\n","            shape=(int(input_shape[1][-1]), self.output_dim),\n","            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n","            trainable=True,\n","        )\n","        self.WV = self.add_weight(\n","            name=\"WV\",\n","            shape=(int(input_shape[2][-1]), self.output_dim),\n","            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n","            trainable=True,\n","        )\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def Mask(self, inputs, seq_len, mode=\"add\"):\n","        \"\"\"Mask operation used in multi-head self attention\n","        Args:\n","            seq_len (object): sequence length of inputs.\n","            mode (str): mode of mask.\n","        Returns:\n","            object: tensors after masking.\n","        \"\"\"\n","\n","        if seq_len is None:\n","            return inputs\n","        else:\n","            mask = K.one_hot(indices=seq_len[:, 0], num_classes=K.shape(inputs)[1])\n","            mask = 1 - K.cumsum(mask, axis=1)\n","\n","            for _ in range(len(inputs.shape) - 2):\n","                mask = K.expand_dims(mask, 2)\n","\n","            if mode == \"mul\":\n","                return inputs * mask\n","            elif mode == \"add\":\n","                return inputs - (1 - mask) * 1e12\n","\n","    def call(self, QKVs):\n","        \"\"\"Core logic of multi-head self attention.\n","        Args:\n","            QKVs (list): inputs of multi-head self attention i.e. query, key and value.\n","        Returns:\n","            object: ouput tensors.\n","        \"\"\"\n","        if len(QKVs) == 3:\n","            Q_seq, K_seq, V_seq = QKVs\n","            Q_len, V_len = None, None\n","        elif len(QKVs) == 5:\n","            Q_seq, K_seq, V_seq, Q_len, V_len = QKVs\n","        Q_seq = K.dot(Q_seq, self.WQ)\n","        Q_seq = K.reshape(\n","            Q_seq, shape=(-1, K.shape(Q_seq)[1], self.multiheads, self.head_dim)\n","        )\n","        Q_seq = K.permute_dimensions(Q_seq, pattern=(0, 2, 1, 3))\n","\n","        K_seq = K.dot(K_seq, self.WK)\n","        K_seq = K.reshape(\n","            K_seq, shape=(-1, K.shape(K_seq)[1], self.multiheads, self.head_dim)\n","        )\n","        K_seq = K.permute_dimensions(K_seq, pattern=(0, 2, 1, 3))\n","\n","        V_seq = K.dot(V_seq, self.WV)\n","        V_seq = K.reshape(\n","            V_seq, shape=(-1, K.shape(V_seq)[1], self.multiheads, self.head_dim)\n","        )\n","        V_seq = K.permute_dimensions(V_seq, pattern=(0, 2, 1, 3))\n","\n","        A = einsum(\"abij, abkj -> abik\", Q_seq, K_seq) / K.sqrt(\n","            K.cast(self.head_dim, dtype=\"float32\")\n","        )\n","        A = K.permute_dimensions(\n","            A, pattern=(0, 3, 2, 1)\n","        )  # A.shape=[batch_size,K_sequence_length,Q_sequence_length,self.multiheads]\n","\n","        A = self.Mask(A, V_len, \"add\")\n","        A = K.permute_dimensions(A, pattern=(0, 3, 2, 1))\n","\n","        if self.mask_right:\n","            ones = K.ones_like(A[:1, :1])\n","            lower_triangular = K.tf.matrix_band_part(ones, num_lower=-1, num_upper=0)\n","            mask = (ones - lower_triangular) * 1e12\n","            A = A - mask\n","        A = K.softmax(A)\n","\n","        O_seq = einsum(\"abij, abjk -> abik\", A, V_seq)\n","        O_seq = K.permute_dimensions(O_seq, pattern=(0, 2, 1, 3))\n","\n","        O_seq = K.reshape(O_seq, shape=(-1, K.shape(O_seq)[1], self.output_dim))\n","        O_seq = self.Mask(O_seq, Q_len, \"mul\")\n","        return O_seq\n","\n","    def get_config(self):\n","        \"\"\"add multiheads, multiheads and mask_right into layer config.\n","        Returns:\n","            dict: config of SelfAttention layer.\n","        \"\"\"\n","        config = super(SelfAttention, self).get_config()\n","        config.update(\n","            {\n","                \"multiheads\": self.multiheads,\n","                \"head_dim\": self.head_dim,\n","                \"mask_right\": self.mask_right,\n","            }\n","        )\n","        return config"],"metadata":{"id":"i3oPo6_NqAOE"},"id":"i3oPo6_NqAOE","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1fb8855e","metadata":{"id":"1fb8855e"},"source":["### NRMS"]},{"cell_type":"code","execution_count":null,"id":"99b31e06","metadata":{"id":"99b31e06"},"outputs":[],"source":["__all__ = [\"NRMSModel\"]\n","\n","\n","class NRMSModel(BaseModel):\n","    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\n","    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n","    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n","    on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n","    on Natural Language Processing (EMNLP-IJCNLP)\n","    Attributes:\n","        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\n","        hparam (object): Global hyper-parameters.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        hparams,\n","        iterator_creator,\n","        seed=None,\n","    ):\n","        \"\"\"Initialization steps for NRMS.\n","        Compared with the BaseModel, NRMS need word embedding.\n","        After creating word embedding matrix, BaseModel's __init__ method will be called.\n","        Args:\n","            hparams (object): Global hyper-parameters. Some key setttings such as head_num and head_dim are there.\n","            iterator_creator_train (object): NRMS data loader class for train data.\n","            iterator_creator_test (object): NRMS data loader class for test and validation data\n","        \"\"\"\n","        self.word2vec_embedding = self._init_embedding(hparams.wordEmb_file)\n","\n","        super().__init__(\n","            hparams,\n","            iterator_creator,\n","            seed=seed,\n","        )\n","\n","    def _get_input_label_from_iter(self, batch_data):\n","        \"\"\"get input and labels for trainning from iterator\n","        Args:\n","            batch data: input batch data from iterator\n","        Returns:\n","            list: input feature fed into model (clicked_title_batch & candidate_title_batch)\n","            numpy.ndarray: labels\n","        \"\"\"\n","        input_feat = [\n","            batch_data[\"clicked_title_batch\"],\n","            batch_data[\"candidate_title_batch\"],\n","        ]\n","        input_label = batch_data[\"labels\"]\n","        return input_feat, input_label\n","\n","    def _get_user_feature_from_iter(self, batch_data):\n","        \"\"\"get input of user encoder\n","        Args:\n","            batch_data: input batch data from user iterator\n","        Returns:\n","            numpy.ndarray: input user feature (clicked title batch)\n","        \"\"\"\n","        return batch_data[\"clicked_title_batch\"]\n","\n","    def _get_news_feature_from_iter(self, batch_data):\n","        \"\"\"get input of news encoder\n","        Args:\n","            batch_data: input batch data from news iterator\n","        Returns:\n","            numpy.ndarray: input news feature (candidate title batch)\n","        \"\"\"\n","        return batch_data[\"candidate_title_batch\"]\n","\n","    def _build_graph(self):\n","        \"\"\"Build NRMS model and scorer.\n","        Returns:\n","            object: a model used to train.\n","            object: a model used to evaluate and inference.\n","        \"\"\"\n","        model, scorer = self._build_nrms()\n","        return model, scorer\n","\n","    def _build_userencoder(self, titleencoder):\n","        \"\"\"The main function to create user encoder of NRMS.\n","        Args:\n","            titleencoder (object): the news encoder of NRMS.\n","        Return:\n","            object: the user encoder of NRMS.\n","        \"\"\"\n","        hparams = self.hparams\n","        his_input_title = keras.Input(\n","            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n","        )\n","\n","        click_title_presents = layers.TimeDistributed(titleencoder)(his_input_title)\n","        y = SelfAttention(hparams.head_num, hparams.head_dim, seed=self.seed)(\n","            [click_title_presents] * 3\n","        )\n","        user_present = AttLayer2(hparams.attention_hidden_dim, seed=self.seed)(y)\n","\n","        model = keras.Model(his_input_title, user_present, name=\"user_encoder\")\n","        return model\n","\n","    def _build_newsencoder(self, embedding_layer):\n","        \"\"\"The main function to create news encoder of NRMS.\n","        Args:\n","            embedding_layer (object): a word embedding layer.\n","        Return:\n","            object: the news encoder of NRMS.\n","        \"\"\"\n","        hparams = self.hparams\n","        sequences_input_title = keras.Input(shape=(hparams.title_size,), dtype=\"int32\")\n","\n","        embedded_sequences_title = embedding_layer(sequences_input_title)\n","\n","        y = layers.Dropout(hparams.dropout)(embedded_sequences_title)\n","        y = SelfAttention(hparams.head_num, hparams.head_dim, seed=self.seed)([y, y, y])\n","        y = layers.Dropout(hparams.dropout)(y)\n","        pred_title = AttLayer2(hparams.attention_hidden_dim, seed=self.seed)(y)\n","\n","        model = keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n","        return model\n","\n","    def _build_nrms(self):\n","        \"\"\"The main function to create NRMS's logic. The core of NRMS\n","        is a user encoder and a news encoder.\n","        Returns:\n","            object: a model used to train.\n","            object: a model used to evaluate and inference.\n","        \"\"\"\n","        hparams = self.hparams\n","\n","        his_input_title = keras.Input(\n","            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n","        )\n","        pred_input_title = keras.Input(\n","            shape=(hparams.npratio + 1, hparams.title_size), dtype=\"int32\"\n","        )\n","        pred_input_title_one = keras.Input(\n","            shape=(\n","                1,\n","                hparams.title_size,\n","            ),\n","            dtype=\"int32\",\n","        )\n","        pred_title_one_reshape = layers.Reshape((hparams.title_size,))(\n","            pred_input_title_one\n","        )\n","\n","        embedding_layer = layers.Embedding(\n","            self.word2vec_embedding.shape[0],\n","            hparams.word_emb_dim,\n","            weights=[self.word2vec_embedding],\n","            trainable=True,\n","        )\n","\n","        titleencoder = self._build_newsencoder(embedding_layer)\n","        self.userencoder = self._build_userencoder(titleencoder)\n","        self.newsencoder = titleencoder\n","\n","        user_present = self.userencoder(his_input_title)\n","        news_present = layers.TimeDistributed(self.newsencoder)(pred_input_title)\n","        news_present_one = self.newsencoder(pred_title_one_reshape)\n","\n","        preds = layers.Dot(axes=-1)([news_present, user_present])\n","        preds = layers.Activation(activation=\"softmax\")(preds)\n","\n","        pred_one = layers.Dot(axes=-1)([news_present_one, user_present])\n","        pred_one = layers.Activation(activation=\"sigmoid\")(pred_one)\n","\n","        model = keras.Model([his_input_title, pred_input_title], preds)\n","        scorer = keras.Model([his_input_title, pred_input_title_one], pred_one)\n","\n","        return model, scorer"]},{"cell_type":"markdown","id":"f3146482","metadata":{"id":"f3146482"},"source":["### Iterator"]},{"cell_type":"code","execution_count":null,"id":"bc1ce578","metadata":{"id":"bc1ce578"},"outputs":[],"source":["class BaseIterator(object):\n","    \"\"\"Abstract base iterator class\"\"\"\n","\n","    @abc.abstractmethod\n","    def parser_one_line(self, line):\n","        \"\"\"Abstract method. Parse one string line into feature values.\n","        Args:\n","            line (str): A string indicating one instance.\n","        \"\"\"\n","        pass\n","\n","    @abc.abstractmethod\n","    def load_data_from_file(self, infile):\n","        \"\"\"Abstract method. Read and parse data from a file.\n","        Args:\n","            infile (str): Text input file. Each line in this file is an instance.\n","        \"\"\"\n","        pass\n","\n","    @abc.abstractmethod\n","    def _convert_data(self, labels, features):\n","        pass\n","\n","    @abc.abstractmethod\n","    def gen_feed_dict(self, data_dict):\n","        \"\"\"Abstract method. Construct a dictionary that maps graph elements to values.\n","        Args:\n","            data_dict (dict): A dictionary that maps string name to numpy arrays.\n","        \"\"\"\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"4c962842","metadata":{"id":"4c962842"},"outputs":[],"source":["class MINDIterator(BaseIterator):\n","    \"\"\"Train data loader for NAML model.\n","    The model require a special type of data format, where each instance contains a label, impresion id, user id,\n","    the candidate news articles and user's clicked news article. Articles are represented by title words,\n","    body words, verts and subverts.\n","    Iterator will not load the whole data into memory. Instead, it loads data into memory\n","    per mini-batch, so that large files can be used as input data.\n","    Attributes:\n","        col_spliter (str): column spliter in one line.\n","        ID_spliter (str): ID spliter in one line.\n","        batch_size (int): the samples num in one batch.\n","        title_size (int): max word num in news title.\n","        his_size (int): max clicked news num in user click history.\n","        npratio (int): negaive and positive ratio used in negative sampling. -1 means no need of negtive sampling.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        hparams,\n","        npratio=-1,\n","        col_spliter=\"\\t\",\n","        ID_spliter=\"%\",\n","    ):\n","        \"\"\"Initialize an iterator. Create necessary placeholders for the model.\n","        Args:\n","            hparams (object): Global hyper-parameters. Some key setttings such as head_num and head_dim are there.\n","            npratio (int): negaive and positive ratio used in negative sampling. -1 means no need of negtive sampling.\n","            col_spliter (str): column spliter in one line.\n","            ID_spliter (str): ID spliter in one line.\n","        \"\"\"\n","        self.col_spliter = col_spliter\n","        self.ID_spliter = ID_spliter\n","        self.batch_size = hparams.batch_size\n","        self.title_size = hparams.title_size\n","        self.his_size = hparams.his_size\n","        self.npratio = npratio\n","\n","        self.word_dict = self.load_dict(hparams.wordDict_file)\n","        self.uid2index = self.load_dict(hparams.userDict_file)\n","\n","    def load_dict(self, file_path):\n","        \"\"\"load pickle file\n","        Args:\n","            file path (str): file path\n","        Returns:\n","            object: pickle loaded object\n","        \"\"\"\n","        with open(file_path, \"rb\") as f:\n","            return pkl.load(f)\n","\n","    def init_news(self, news_file):\n","        \"\"\"init news information given news file, such as news_title_index and nid2index.\n","        Args:\n","            news_file: path of news file\n","        \"\"\"\n","\n","        self.nid2index = {}\n","        news_title = [\"\"]\n","\n","        with tf.io.gfile.GFile(news_file, \"r\") as rd:\n","            for line in rd:\n","                nid, vert, subvert, title = line.strip(\"\\n\").split(\n","                    self.col_spliter\n","                )\n","\n","                if nid in self.nid2index:\n","                    continue\n","\n","                self.nid2index[nid] = len(self.nid2index) + 1\n","                title = word_tokenize(title)\n","                news_title.append(title)\n","\n","        self.news_title_index = np.zeros(\n","            (len(news_title), self.title_size), dtype=\"int32\"\n","        )\n","\n","        for news_index in range(len(news_title)):\n","            title = news_title[news_index]\n","            for word_index in range(min(self.title_size, len(title))):\n","                if title[word_index] in self.word_dict:\n","                    self.news_title_index[news_index, word_index] = self.word_dict[\n","                        title[word_index].lower()\n","                    ]\n","\n","    def init_behaviors(self, behaviors_file):\n","        \"\"\"init behavior logs given behaviors file.\n","        Args:\n","        behaviors_file: path of behaviors file\n","        \"\"\"\n","        self.histories = []\n","        self.imprs = []\n","        self.labels = []\n","        self.impr_indexes = []\n","        self.uindexes = []\n","\n","        with tf.io.gfile.GFile(behaviors_file, \"r\") as rd:\n","            impr_index = 0\n","            for line in rd:\n","                uid, time, history, impr = line.strip(\"\\n\").split(self.col_spliter)[-4:]\n","\n","                history = [self.nid2index[i] for i in history.split()]\n","                history = [0] * (self.his_size - len(history)) + history[\n","                    : self.his_size\n","                ]\n","\n","                impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n","                label = [int(i.split(\"-\")[1]) for i in impr.split()]\n","                uindex = self.uid2index[uid] if uid in self.uid2index else 0\n","\n","                self.histories.append(history)\n","                self.imprs.append(impr_news)\n","                self.labels.append(label)\n","                self.impr_indexes.append(impr_index)\n","                self.uindexes.append(uindex)\n","                impr_index += 1\n","\n","    def parser_one_line(self, line):\n","        \"\"\"Parse one behavior sample into feature values.\n","        if npratio is larger than 0, return negtive sampled result.\n","        Args:\n","            line (int): sample index.\n","        Yields:\n","            list: Parsed results including label, impression id , user id,\n","            candidate_title_index, clicked_title_index.\n","        \"\"\"\n","        if self.npratio > 0:\n","            impr_label = self.labels[line]\n","            impr = self.imprs[line]\n","\n","            poss = []\n","            negs = []\n","\n","            for news, click in zip(impr, impr_label):\n","                if click == 1:\n","                    poss.append(news)\n","                else:\n","                    negs.append(news)\n","\n","            for p in poss:\n","                candidate_title_index = []\n","                impr_index = []\n","                user_index = []\n","                label = [1] + [0] * self.npratio\n","\n","                n = newsample(negs, self.npratio)\n","                candidate_title_index = self.news_title_index[[p] + n]\n","                click_title_index = self.news_title_index[self.histories[line]]\n","                impr_index.append(self.impr_indexes[line])\n","                user_index.append(self.uindexes[line])\n","\n","                yield (\n","                    label,\n","                    impr_index,\n","                    user_index,\n","                    candidate_title_index,\n","                    click_title_index,\n","                )\n","\n","        else:\n","            impr_label = self.labels[line]\n","            impr = self.imprs[line]\n","\n","            for news, label in zip(impr, impr_label):\n","                candidate_title_index = []\n","                impr_index = []\n","                user_index = []\n","                label = [label]\n","\n","                candidate_title_index.append(self.news_title_index[news])\n","                click_title_index = self.news_title_index[self.histories[line]]\n","                impr_index.append(self.impr_indexes[line])\n","                user_index.append(self.uindexes[line])\n","\n","                yield (\n","                    label,\n","                    impr_index,\n","                    user_index,\n","                    candidate_title_index,\n","                    click_title_index,\n","                )\n","\n","    def load_data_from_file(self, news_file, behavior_file):\n","        \"\"\"Read and parse data from news file and behavior file.\n","        Args:\n","            news_file (str): A file contains several informations of news.\n","            beahaviros_file (str): A file contains information of user impressions.\n","        Yields:\n","            object: An iterator that yields parsed results, in the format of dict.\n","        \"\"\"\n","\n","        if not hasattr(self, \"news_title_index\"):\n","            self.init_news(news_file)\n","\n","        if not hasattr(self, \"impr_indexes\"):\n","            self.init_behaviors(behavior_file)\n","\n","        label_list = []\n","        imp_indexes = []\n","        user_indexes = []\n","        candidate_title_indexes = []\n","        click_title_indexes = []\n","        cnt = 0\n","\n","        indexes = np.arange(len(self.labels))\n","\n","        if self.npratio > 0:\n","            np.random.shuffle(indexes)\n","\n","        for index in indexes:\n","            for (\n","                label,\n","                imp_index,\n","                user_index,\n","                candidate_title_index,\n","                click_title_index,\n","            ) in self.parser_one_line(index):\n","                candidate_title_indexes.append(candidate_title_index)\n","                click_title_indexes.append(click_title_index)\n","                imp_indexes.append(imp_index)\n","                user_indexes.append(user_index)\n","                label_list.append(label)\n","\n","                cnt += 1\n","                if cnt >= self.batch_size:\n","                    yield self._convert_data(\n","                        label_list,\n","                        imp_indexes,\n","                        user_indexes,\n","                        candidate_title_indexes,\n","                        click_title_indexes,\n","                    )\n","                    label_list = []\n","                    imp_indexes = []\n","                    user_indexes = []\n","                    candidate_title_indexes = []\n","                    click_title_indexes = []\n","                    cnt = 0\n","\n","        if cnt > 0:\n","            yield self._convert_data(\n","                label_list,\n","                imp_indexes,\n","                user_indexes,\n","                candidate_title_indexes,\n","                click_title_indexes,\n","            )\n","\n","    def _convert_data(\n","        self,\n","        label_list,\n","        imp_indexes,\n","        user_indexes,\n","        candidate_title_indexes,\n","        click_title_indexes,\n","    ):\n","        \"\"\"Convert data into numpy arrays that are good for further model operation.\n","        Args:\n","            label_list (list): a list of ground-truth labels.\n","            imp_indexes (list): a list of impression indexes.\n","            user_indexes (list): a list of user indexes.\n","            candidate_title_indexes (list): the candidate news titles' words indices.\n","            click_title_indexes (list): words indices for user's clicked news titles.\n","        Returns:\n","            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n","        \"\"\"\n","\n","        labels = np.asarray(label_list, dtype=np.float32)\n","        imp_indexes = np.asarray(imp_indexes, dtype=np.int32)\n","        user_indexes = np.asarray(user_indexes, dtype=np.int32)\n","        candidate_title_index_batch = np.asarray(\n","            candidate_title_indexes, dtype=np.int64\n","        )\n","        click_title_index_batch = np.asarray(click_title_indexes, dtype=np.int64)\n","        return {\n","            \"impression_index_batch\": imp_indexes,\n","            \"user_index_batch\": user_indexes,\n","            \"clicked_title_batch\": click_title_index_batch,\n","            \"candidate_title_batch\": candidate_title_index_batch,\n","            \"labels\": labels,\n","        }\n","\n","    def load_user_from_file(self, news_file, behavior_file):\n","        \"\"\"Read and parse user data from news file and behavior file.\n","        Args:\n","            news_file (str): A file contains several informations of news.\n","            beahaviros_file (str): A file contains information of user impressions.\n","        Yields:\n","            object: An iterator that yields parsed user feature, in the format of dict.\n","        \"\"\"\n","\n","        if not hasattr(self, \"news_title_index\"):\n","            self.init_news(news_file)\n","\n","        if not hasattr(self, \"impr_indexes\"):\n","            self.init_behaviors(behavior_file)\n","\n","        user_indexes = []\n","        impr_indexes = []\n","        click_title_indexes = []\n","        cnt = 0\n","\n","        for index in range(len(self.impr_indexes)):\n","            click_title_indexes.append(self.news_title_index[self.histories[index]])\n","            user_indexes.append(self.uindexes[index])\n","            impr_indexes.append(self.impr_indexes[index])\n","\n","            cnt += 1\n","            if cnt >= self.batch_size:\n","                yield self._convert_user_data(\n","                    user_indexes,\n","                    impr_indexes,\n","                    click_title_indexes,\n","                )\n","                user_indexes = []\n","                impr_indexes = []\n","                click_title_indexes = []\n","                cnt = 0\n","\n","        if cnt > 0:\n","            yield self._convert_user_data(\n","                user_indexes,\n","                impr_indexes,\n","                click_title_indexes,\n","            )\n","\n","    def _convert_user_data(\n","        self,\n","        user_indexes,\n","        impr_indexes,\n","        click_title_indexes,\n","    ):\n","        \"\"\"Convert data into numpy arrays that are good for further model operation.\n","        Args:\n","            user_indexes (list): a list of user indexes.\n","            click_title_indexes (list): words indices for user's clicked news titles.\n","        Returns:\n","            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n","        \"\"\"\n","\n","        user_indexes = np.asarray(user_indexes, dtype=np.int32)\n","        impr_indexes = np.asarray(impr_indexes, dtype=np.int32)\n","        click_title_index_batch = np.asarray(click_title_indexes, dtype=np.int64)\n","\n","        return {\n","            \"user_index_batch\": user_indexes,\n","            \"impr_index_batch\": impr_indexes,\n","            \"clicked_title_batch\": click_title_index_batch,\n","        }\n","\n","    def load_news_from_file(self, news_file):\n","        \"\"\"Read and parse user data from news file.\n","        Args:\n","            news_file (str): A file contains several informations of news.\n","        Yields:\n","            object: An iterator that yields parsed news feature, in the format of dict.\n","        \"\"\"\n","        if not hasattr(self, \"news_title_index\"):\n","            self.init_news(news_file)\n","\n","        news_indexes = []\n","        candidate_title_indexes = []\n","        cnt = 0\n","\n","        for index in range(len(self.news_title_index)):\n","            news_indexes.append(index)\n","            candidate_title_indexes.append(self.news_title_index[index])\n","\n","            cnt += 1\n","            if cnt >= self.batch_size:\n","                yield self._convert_news_data(\n","                    news_indexes,\n","                    candidate_title_indexes,\n","                )\n","                news_indexes = []\n","                candidate_title_indexes = []\n","                cnt = 0\n","\n","        if cnt > 0:\n","            yield self._convert_news_data(\n","                news_indexes,\n","                candidate_title_indexes,\n","            )\n","\n","    def _convert_news_data(\n","        self,\n","        news_indexes,\n","        candidate_title_indexes,\n","    ):\n","        \"\"\"Convert data into numpy arrays that are good for further model operation.\n","        Args:\n","            news_indexes (list): a list of news indexes.\n","            candidate_title_indexes (list): the candidate news titles' words indices.\n","        Returns:\n","            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n","        \"\"\"\n","\n","        news_indexes_batch = np.asarray(news_indexes, dtype=np.int32)\n","        candidate_title_index_batch = np.asarray(\n","            candidate_title_indexes, dtype=np.int32\n","        )\n","\n","        return {\n","            \"news_index_batch\": news_indexes_batch,\n","            \"candidate_title_batch\": candidate_title_index_batch,\n","        }\n","\n","    def load_impression_from_file(self, behaivors_file):\n","        \"\"\"Read and parse impression data from behaivors file.\n","        Args:\n","            behaivors_file (str): A file contains several informations of behaviros.\n","        Yields:\n","            object: An iterator that yields parsed impression data, in the format of dict.\n","        \"\"\"\n","\n","        if not hasattr(self, \"histories\"):\n","            self.init_behaviors(behaivors_file)\n","\n","        indexes = np.arange(len(self.labels))\n","\n","        for index in indexes:\n","            impr_label = np.array(self.labels[index], dtype=\"int32\")\n","            impr_news = np.array(self.imprs[index], dtype=\"int32\")\n","\n","            yield (\n","                self.impr_indexes[index],\n","                impr_news,\n","                self.uindexes[index],\n","                impr_label,\n","            )"]},{"cell_type":"code","execution_count":null,"id":"6ee5d799","metadata":{"scrolled":true,"id":"6ee5d799","executionInfo":{"status":"ok","timestamp":1668439631463,"user_tz":-540,"elapsed":6081,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e386b6cb-aaab-4339-afd3-f0b25d46de14"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["# Model init\n","\n","model = NRMSModel(hparams, MINDIterator, seed=seed)"]},{"cell_type":"markdown","id":"b3781b3f","metadata":{"id":"b3781b3f"},"source":["## Train"]},{"cell_type":"code","source":["print(model.run_eval(valid_items_file, valid_behaviors_file))"],"metadata":{"id":"t2aQKXzYqUnO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668439679031,"user_tz":-540,"elapsed":47580,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"a9705a10-ca8d-4477-af72-4f36614d6a98"},"id":"t2aQKXzYqUnO","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","305it [00:03, 97.95it/s] \n","699it [00:23, 29.58it/s]\n","22365it [00:01, 21730.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{'group_auc': 0.4577, 'mean_mrr': 0.1762, 'ndcg@5': 0.1373, 'ndcg@10': 0.1465}\n"]}]},{"cell_type":"code","execution_count":null,"id":"e1192978","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e1192978","executionInfo":{"status":"ok","timestamp":1668444003108,"user_tz":-540,"elapsed":4324100,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"b53b0712-421e-47a8-b6c2-aa11fd155695"},"outputs":[{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.3350, data_loss: 1.2976: : 2453it [06:37,  6.17it/s]\n","305it [00:00, 704.47it/s]\n","699it [00:23, 30.37it/s]\n","22365it [00:01, 21398.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 1\n","train info: logloss loss:1.3350297638556445\n","eval info: group_auc:0.6644, mean_mrr:0.3592, ndcg@10:0.3512, ndcg@5:0.35\n","at epoch 1 , train time: 397.6 eval time: 43.5\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.2690, data_loss: 1.2711: : 2453it [06:29,  6.30it/s]\n","305it [00:00, 690.32it/s]\n","699it [00:22, 30.39it/s]\n","22365it [00:01, 20910.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 2\n","train info: logloss loss:1.2690721255295432\n","eval info: group_auc:0.6681, mean_mrr:0.3684, ndcg@10:0.358, ndcg@5:0.3567\n","at epoch 2 , train time: 389.4 eval time: 43.2\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.2453, data_loss: 1.2538: : 2453it [06:29,  6.30it/s]\n","305it [00:00, 699.06it/s]\n","699it [00:23, 30.35it/s]\n","22365it [00:01, 21778.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 3\n","train info: logloss loss:1.2452655574811802\n","eval info: group_auc:0.6705, mean_mrr:0.3739, ndcg@10:0.3625, ndcg@5:0.3611\n","at epoch 3 , train time: 389.6 eval time: 43.3\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.2332, data_loss: 1.2333: : 2453it [06:28,  6.31it/s]\n","305it [00:00, 728.24it/s]\n","699it [00:22, 30.51it/s]\n","22365it [00:01, 21900.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 4\n","train info: logloss loss:1.2331620984985252\n","eval info: group_auc:0.6726, mean_mrr:0.3779, ndcg@10:0.3652, ndcg@5:0.3637\n","at epoch 4 , train time: 389.0 eval time: 42.9\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.2182, data_loss: 1.1276: : 2453it [06:28,  6.32it/s]\n","305it [00:00, 684.34it/s]\n","699it [00:22, 30.50it/s]\n","22365it [00:01, 21491.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 5\n","train info: logloss loss:1.2181353556395451\n","eval info: group_auc:0.6754, mean_mrr:0.3799, ndcg@10:0.3666, ndcg@5:0.3642\n","at epoch 5 , train time: 388.3 eval time: 43.2\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.2064, data_loss: 1.1869: : 2453it [06:27,  6.33it/s]\n","305it [00:00, 732.31it/s]\n","699it [00:22, 30.46it/s]\n","22365it [00:00, 22536.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 6\n","train info: logloss loss:1.2061731059649405\n","eval info: group_auc:0.6776, mean_mrr:0.3832, ndcg@10:0.3695, ndcg@5:0.3671\n","at epoch 6 , train time: 387.5 eval time: 43.1\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.1943, data_loss: 1.2410: : 2453it [06:27,  6.32it/s]\n","305it [00:00, 703.52it/s]\n","699it [00:22, 30.43it/s]\n","22365it [00:00, 22605.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 7\n","train info: logloss loss:1.1945748708221895\n","eval info: group_auc:0.6791, mean_mrr:0.3837, ndcg@10:0.37, ndcg@5:0.3667\n","at epoch 7 , train time: 387.9 eval time: 42.9\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.1859, data_loss: 1.2450: : 2453it [06:27,  6.33it/s]\n","305it [00:00, 705.81it/s]\n","699it [00:22, 30.49it/s]\n","22365it [00:01, 20996.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 8\n","train info: logloss loss:1.1858221716750557\n","eval info: group_auc:0.6807, mean_mrr:0.3842, ndcg@10:0.3709, ndcg@5:0.3677\n","at epoch 8 , train time: 387.4 eval time: 43.2\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.1801, data_loss: 1.1520: : 2453it [06:27,  6.32it/s]\n","305it [00:00, 713.57it/s]\n","699it [00:23, 30.38it/s]\n","22365it [00:00, 22465.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 9\n","train info: logloss loss:1.1798990880330145\n","eval info: group_auc:0.68, mean_mrr:0.3864, ndcg@10:0.3729, ndcg@5:0.3701\n","at epoch 9 , train time: 387.9 eval time: 43.2\n"]},{"output_type":"stream","name":"stderr","text":["step 2450 , total_loss: 1.1737, data_loss: 1.0101: : 2453it [06:28,  6.32it/s]\n","305it [00:00, 709.05it/s]\n","699it [00:22, 30.43it/s]\n","22365it [00:00, 22475.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["at epoch 10\n","train info: logloss loss:1.173926398844511\n","eval info: group_auc:0.6808, mean_mrr:0.3861, ndcg@10:0.3723, ndcg@5:0.3684\n","at epoch 10 , train time: 388.2 eval time: 43.0\n","CPU times: user 36min 47s, sys: 2min 1s, total: 38min 48s\n","Wall time: 1h 12min 4s\n"]},{"output_type":"execute_result","data":{"text/plain":["<__main__.NRMSModel at 0x7f0060dfadd0>"]},"metadata":{},"execution_count":28}],"source":["%%time\n","model.fit(train_items_file, train_behaviors_file, valid_items_file, valid_behaviors_file)"]},{"cell_type":"code","execution_count":null,"id":"d912a891","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d912a891","executionInfo":{"status":"ok","timestamp":1668444525749,"user_tz":-540,"elapsed":40991,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"56a0ff2a-ec9d-4c36-b67c-ba44571080eb"},"outputs":[{"output_type":"stream","name":"stderr","text":["305it [00:00, 551.15it/s]\n","699it [00:20, 33.41it/s]\n","22365it [00:00, 22945.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["{'group_auc': 0.6808, 'mean_mrr': 0.3861, 'ndcg@5': 0.3684, 'ndcg@10': 0.3723}\n","CPU times: user 42 s, sys: 845 ms, total: 42.8 s\n","Wall time: 40.8 s\n"]}],"source":["%%time\n","res_syn = model.run_eval(valid_items_file, valid_behaviors_file)\n","print(res_syn)"]},{"cell_type":"markdown","id":"c156d1ab","metadata":{"id":"c156d1ab"},"source":["## Prediction"]},{"cell_type":"markdown","source":["### Calculation"],"metadata":{"id":"8JRNqUUsvYe-"},"id":"8JRNqUUsvYe-"},{"cell_type":"code","execution_count":null,"id":"c4c7d648","metadata":{"id":"c4c7d648"},"outputs":[],"source":["class NewsRecCal:\n","    def __init__(self, model, hparams, news_file, behaviors_file, k=18720):\n","        \"\"\"Initialize an iterator. Create necessary placeholders for the model.\n","        Args:\n","            model (object): Pre-trained NewsRec Model.\n","            hparams (object) : Pre-defined hyperparameters & configurations\n","            news_file (file directory + name): News tsv file of the MIND dataset format.\n","            behaviors_file (file directory + name): Behavior tsv file of the MIND dataset format.\n","            k (int): Number of maxiumum rank to recommend\n","        \"\"\"\n","        self.model = model\n","        self.hparams = hparams\n","        self.news_file = news_file\n","        self.behaviors_file = behaviors_file\n","        self.news = pd.read_csv(self.news_file, sep=\"\\t\",header=None)\n","        self.behav = pd.read_csv(self.behaviors_file, sep=\"\\t\",header=None)\n","        self.k = k\n","        \n","        # 각 번호에 해당하는 뉴스가 어떤 뉴스인지 복원 위해 필요\n","        self.test_iterator = MINDIterator(hparams, col_spliter=\"\\t\")\n","        self.test_iterator.init_news(self.news_file)\n","    \n","    \n","        # 다음으로, news_vecs와 user_vecs를 각각 model.run_news()와 model.run_user()로부터 가져옴\n","        self.news_vecs = self.model.run_news(self.news_file)\n","        self.user_vecs = self.model.run_user(self.news_file,self.behaviors_file)\n","    \n","    \n","    def preprocess(self):        \n","        \"\"\"\n","        user_vecs의 길이가 user의 unique 수가 아니라, impression의 수이고, \n","        따라서 user_vecs 내에 중복 벡터들이 있으므로, unique한 user벡터만 남기는 작업부터 해 주겠음!\n","        \"\"\"\n","        # unique한 user벡터를 user_vecs_arr로 저장\n","        duplicate_dropped_idx = np.array(pd.DataFrame(self.behav[0]).drop_duplicates().index)\n","        user_vecs_arr = pd.DataFrame(self.user_vecs)[duplicate_dropped_idx].transpose().values\n","    \n","    \n","        # news_vecs 역시 dictionary에서 array로 변환\n","        news_vecs_arr = pd.DataFrame(self.news_vecs).transpose().values\n","        \n","        \n","        # garbage value인 news_vec의 0번째 element와의 내적 결과 제거하고 dataframe 구성 위해 필요\n","        user_keys = self.behav.loc[duplicate_dropped_idx][0].to_list()\n","        news_keys = list(self.test_iterator.nid2index.keys())\n","        \n","        return user_vecs_arr, news_vecs_arr, user_keys, news_keys\n","\n","    \n","    def score(self):\n","        self.user_vecs_arr, self.news_vecs_arr, self.user_keys, self.news_keys = self.preprocess()\n","        \n","        # 각 unique user별 각 news와의 내적 값을 행렬곱 형태로 계산\n","        user_news_score = np.matmul(self.user_vecs_arr,self.news_vecs_arr.transpose())\n","\n","        score_df = pd.DataFrame(user_news_score[:,1:],index=pd.Index(self.user_keys))\n","        score_df.columns = self.news_keys\n","\n","        return score_df\n","    \n","    \n","    def recommend(self):    \n","        # 각 user별로, score 낮은 뉴스부터 높은 뉴스 순으로 \n","        self.score_df = self.score()\n","        argsort_df = np.argsort(self.score_df)\n","        rank_df = pd.DataFrame(np.array(self.news_keys)[argsort_df.values],index=self.score_df.index)\n","\n","\n","        # 각 user별로, score 높은 뉴스부터 낮은 뉴스 순으로\n","        # 칼럼명은 추천 순위\n","        rank_df = rank_df[rank_df.columns[::-1]]\n","        rank_df.columns = rank_df.columns[::-1]+1\n","\n","        return rank_df.loc[:,:self.k]"]},{"cell_type":"code","execution_count":null,"id":"e41fddcc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e41fddcc","executionInfo":{"status":"ok","timestamp":1668444548748,"user_tz":-540,"elapsed":23035,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"97c1c0be-930f-442d-e6ae-8ba171111900"},"outputs":[{"output_type":"stream","name":"stderr","text":["305it [00:00, 558.40it/s]\n","699it [00:21, 32.96it/s]"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 23.7 s, sys: 513 ms, total: 24.2 s\n","Wall time: 22.6 s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["%%time\n","calculator = NewsRecCal(model, hparams, valid_items_file, valid_behaviors_file)"]},{"cell_type":"code","execution_count":null,"id":"391427ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"391427ca","executionInfo":{"status":"ok","timestamp":1668444549151,"user_tz":-540,"elapsed":448,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"b2d499e2-e2e9-4380-ccda-0fc36ab27ff1"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 465 ms, sys: 3.26 ms, total: 469 ms\n","Wall time: 451 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["            1         2         3         4         5         6         7  \\\n","489  2.667581  2.397735  1.283872 -0.591967  0.308801 -0.591967  0.174539   \n","490  3.446257  2.831138  0.354795 -0.251179 -0.132704 -0.251179  0.141553   \n","491  3.334001  2.862745  0.823628 -0.429460  0.042003 -0.429460  0.179201   \n","492  3.853889  1.779387  2.242132 -1.056541  0.931124 -1.056541  0.620227   \n","493  4.102018  3.203485  1.826670 -0.925672  0.451614 -0.925672  0.624760   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","606  2.604634  2.606436  1.966890 -0.775747  1.020938 -0.775747  0.459397   \n","607  4.160751  2.620541  2.551383 -1.216149  0.975861 -1.216149  0.712862   \n","608  3.322134  2.998920  1.523911 -0.748143  0.550090 -0.748143  0.439682   \n","609  4.508835  1.210237  2.785821 -1.364195  1.401688 -1.364195  0.885334   \n","610  4.438873  4.017978  1.088084 -0.476002  0.035190 -0.476002  0.306863   \n","\n","            8         9        10  ...    193565    193567    193571  \\\n","489 -0.591967 -0.591967 -0.591967  ... -0.020322 -0.591967 -0.591967   \n","490 -0.251179 -0.251179 -0.251179  ...  0.057519 -0.251179 -0.251179   \n","491 -0.429460 -0.429460 -0.429460  ... -0.060634 -0.429460 -0.429460   \n","492 -1.056541 -1.056541 -1.056541  ... -0.663384 -1.056541 -1.056541   \n","493 -0.925672 -0.925672 -0.925672  ... -0.577664 -0.925672 -0.925672   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","606 -0.775747 -0.775747 -0.775747  ...  0.247178 -0.775747 -0.775747   \n","607 -1.216149 -1.216149 -1.216149  ... -0.720383 -1.216149 -1.216149   \n","608 -0.748143 -0.748143 -0.748143  ... -0.239732 -0.748143 -0.748143   \n","609 -1.364195 -1.364195 -1.364195  ... -1.281911 -1.364195 -1.364195   \n","610 -0.476002 -0.476002 -0.476002  ...  0.003612 -0.476002 -0.476002   \n","\n","       193573    193579    193581    193583    193585    193587    193609  \n","489 -0.591967 -0.591967 -0.591967 -0.591967 -0.591967 -0.591967 -0.591967  \n","490 -0.251179 -0.251179 -0.251179 -0.251179 -0.251179 -0.251179 -0.251179  \n","491 -0.429460 -0.429460 -0.429460 -0.429460 -0.429460 -0.429460 -0.429460  \n","492 -1.056541 -1.056541 -1.056541 -1.056541 -1.056541 -1.056541 -1.056541  \n","493 -0.925672 -0.925672 -0.925672 -0.925672 -0.925672 -0.925672 -0.925672  \n","..        ...       ...       ...       ...       ...       ...       ...  \n","606 -0.775747 -0.775747 -0.775747 -0.775747 -0.775747 -0.775747 -0.775747  \n","607 -1.216149 -1.216149 -1.216149 -1.216149 -1.216149 -1.216149 -1.216149  \n","608 -0.748143 -0.748143 -0.748143 -0.748143 -0.748143 -0.748143 -0.748143  \n","609 -1.364195 -1.364195 -1.364195 -1.364195 -1.364195 -1.364195 -1.364195  \n","610 -0.476002 -0.476002 -0.476002 -0.476002 -0.476002 -0.476002 -0.476002  \n","\n","[122 rows x 9742 columns]"],"text/html":["\n","  <div id=\"df-0ba23ade-a77d-449f-a346-384d0beaff8e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>...</th>\n","      <th>193565</th>\n","      <th>193567</th>\n","      <th>193571</th>\n","      <th>193573</th>\n","      <th>193579</th>\n","      <th>193581</th>\n","      <th>193583</th>\n","      <th>193585</th>\n","      <th>193587</th>\n","      <th>193609</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>489</th>\n","      <td>2.667581</td>\n","      <td>2.397735</td>\n","      <td>1.283872</td>\n","      <td>-0.591967</td>\n","      <td>0.308801</td>\n","      <td>-0.591967</td>\n","      <td>0.174539</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>...</td>\n","      <td>-0.020322</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","      <td>-0.591967</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>3.446257</td>\n","      <td>2.831138</td>\n","      <td>0.354795</td>\n","      <td>-0.251179</td>\n","      <td>-0.132704</td>\n","      <td>-0.251179</td>\n","      <td>0.141553</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>...</td>\n","      <td>0.057519</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","      <td>-0.251179</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>3.334001</td>\n","      <td>2.862745</td>\n","      <td>0.823628</td>\n","      <td>-0.429460</td>\n","      <td>0.042003</td>\n","      <td>-0.429460</td>\n","      <td>0.179201</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>...</td>\n","      <td>-0.060634</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","      <td>-0.429460</td>\n","    </tr>\n","    <tr>\n","      <th>492</th>\n","      <td>3.853889</td>\n","      <td>1.779387</td>\n","      <td>2.242132</td>\n","      <td>-1.056541</td>\n","      <td>0.931124</td>\n","      <td>-1.056541</td>\n","      <td>0.620227</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>...</td>\n","      <td>-0.663384</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","      <td>-1.056541</td>\n","    </tr>\n","    <tr>\n","      <th>493</th>\n","      <td>4.102018</td>\n","      <td>3.203485</td>\n","      <td>1.826670</td>\n","      <td>-0.925672</td>\n","      <td>0.451614</td>\n","      <td>-0.925672</td>\n","      <td>0.624760</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>...</td>\n","      <td>-0.577664</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","      <td>-0.925672</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>606</th>\n","      <td>2.604634</td>\n","      <td>2.606436</td>\n","      <td>1.966890</td>\n","      <td>-0.775747</td>\n","      <td>1.020938</td>\n","      <td>-0.775747</td>\n","      <td>0.459397</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>...</td>\n","      <td>0.247178</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","      <td>-0.775747</td>\n","    </tr>\n","    <tr>\n","      <th>607</th>\n","      <td>4.160751</td>\n","      <td>2.620541</td>\n","      <td>2.551383</td>\n","      <td>-1.216149</td>\n","      <td>0.975861</td>\n","      <td>-1.216149</td>\n","      <td>0.712862</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>...</td>\n","      <td>-0.720383</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","      <td>-1.216149</td>\n","    </tr>\n","    <tr>\n","      <th>608</th>\n","      <td>3.322134</td>\n","      <td>2.998920</td>\n","      <td>1.523911</td>\n","      <td>-0.748143</td>\n","      <td>0.550090</td>\n","      <td>-0.748143</td>\n","      <td>0.439682</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>...</td>\n","      <td>-0.239732</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","      <td>-0.748143</td>\n","    </tr>\n","    <tr>\n","      <th>609</th>\n","      <td>4.508835</td>\n","      <td>1.210237</td>\n","      <td>2.785821</td>\n","      <td>-1.364195</td>\n","      <td>1.401688</td>\n","      <td>-1.364195</td>\n","      <td>0.885334</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>...</td>\n","      <td>-1.281911</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","      <td>-1.364195</td>\n","    </tr>\n","    <tr>\n","      <th>610</th>\n","      <td>4.438873</td>\n","      <td>4.017978</td>\n","      <td>1.088084</td>\n","      <td>-0.476002</td>\n","      <td>0.035190</td>\n","      <td>-0.476002</td>\n","      <td>0.306863</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>...</td>\n","      <td>0.003612</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","      <td>-0.476002</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>122 rows × 9742 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ba23ade-a77d-449f-a346-384d0beaff8e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0ba23ade-a77d-449f-a346-384d0beaff8e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0ba23ade-a77d-449f-a346-384d0beaff8e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":41}],"source":["%%time\n","score_df = calculator.score() # 함수 버전과 마찬가지로, 19초만에 전체 추천 결과 도출!\n","score_df"]},{"cell_type":"code","execution_count":null,"id":"ef71b5c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef71b5c1","executionInfo":{"status":"ok","timestamp":1668444549505,"user_tz":-540,"elapsed":365,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"af4b28ea-36c2-47c2-8e54-d2714dc28802"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 663 ms, sys: 186 ms, total: 850 ms\n","Wall time: 661 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["      1     2     3     4      5      6      7      8     9     10    ...  \\\n","489    356   296  2959  5952    593   4878    110   5445   293   608  ...   \n","490  79132  5952  2959   260  72998  99114     32   7153  5445   356  ...   \n","491   2959   296   260   356   1196   5952     32  79132   110   293  ...   \n","492    480   380  2355   356   1196    296    593    377   589  1210  ...   \n","493    480   260  1196   589    380    356   1210    593   296    32  ...   \n","..     ...   ...   ...   ...    ...    ...    ...    ...   ...   ...  ...   \n","606    296   356   593  2959    293   1196    110    608  4878   480  ...   \n","607    480   380   356   593   1196    296    589   2355   260  1210  ...   \n","608    480   296   356   260   1196    110   2959    589   293   593  ...   \n","609   2355   480   380   587      1    185    356    593   296   805  ...   \n","610   5952  2959   356   260     32    296  79132    593   110  2571  ...   \n","\n","       9733    9734    9735    9736    9737    9738    9739    9740    9741  \\\n","489    6770  127172    6232  148881    7061   30892   26717  176419  153070   \n","490    4046    6232  176419   30892    6669   26717  153070    6770    7061   \n","491    8938    6669  148881   26717    6770   30892    7061  153070  176419   \n","492  176419  122912  135536  104863  112421  187593    6818  119145   68848   \n","493  187593  104863   99917   57502    6818   68848  176419  119145  112421   \n","..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","606  156605   30892  104863   68848   26717    6818  148881  153070  176419   \n","607  135536  122912  112421  176419  104863  187593    6818   68848  119145   \n","608    6818  112421  187593  148881   68848   57502   30892  104863  153070   \n","609  167746  104863  158966    6818  135536  122912  187593   68848  119145   \n","610  127172    6232   30892  176419    6669   26717    6770  153070    7061   \n","\n","       9742  \n","489   57502  \n","490   57502  \n","491   57502  \n","492  138036  \n","493  138036  \n","..      ...  \n","606   57502  \n","607  138036  \n","608  176419  \n","609  138036  \n","610   57502  \n","\n","[122 rows x 9742 columns]"],"text/html":["\n","  <div id=\"df-a5aaf5ea-82a9-4a24-9c6b-48c5f711c2e6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>...</th>\n","      <th>9733</th>\n","      <th>9734</th>\n","      <th>9735</th>\n","      <th>9736</th>\n","      <th>9737</th>\n","      <th>9738</th>\n","      <th>9739</th>\n","      <th>9740</th>\n","      <th>9741</th>\n","      <th>9742</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>489</th>\n","      <td>356</td>\n","      <td>296</td>\n","      <td>2959</td>\n","      <td>5952</td>\n","      <td>593</td>\n","      <td>4878</td>\n","      <td>110</td>\n","      <td>5445</td>\n","      <td>293</td>\n","      <td>608</td>\n","      <td>...</td>\n","      <td>6770</td>\n","      <td>127172</td>\n","      <td>6232</td>\n","      <td>148881</td>\n","      <td>7061</td>\n","      <td>30892</td>\n","      <td>26717</td>\n","      <td>176419</td>\n","      <td>153070</td>\n","      <td>57502</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>79132</td>\n","      <td>5952</td>\n","      <td>2959</td>\n","      <td>260</td>\n","      <td>72998</td>\n","      <td>99114</td>\n","      <td>32</td>\n","      <td>7153</td>\n","      <td>5445</td>\n","      <td>356</td>\n","      <td>...</td>\n","      <td>4046</td>\n","      <td>6232</td>\n","      <td>176419</td>\n","      <td>30892</td>\n","      <td>6669</td>\n","      <td>26717</td>\n","      <td>153070</td>\n","      <td>6770</td>\n","      <td>7061</td>\n","      <td>57502</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>2959</td>\n","      <td>296</td>\n","      <td>260</td>\n","      <td>356</td>\n","      <td>1196</td>\n","      <td>5952</td>\n","      <td>32</td>\n","      <td>79132</td>\n","      <td>110</td>\n","      <td>293</td>\n","      <td>...</td>\n","      <td>8938</td>\n","      <td>6669</td>\n","      <td>148881</td>\n","      <td>26717</td>\n","      <td>6770</td>\n","      <td>30892</td>\n","      <td>7061</td>\n","      <td>153070</td>\n","      <td>176419</td>\n","      <td>57502</td>\n","    </tr>\n","    <tr>\n","      <th>492</th>\n","      <td>480</td>\n","      <td>380</td>\n","      <td>2355</td>\n","      <td>356</td>\n","      <td>1196</td>\n","      <td>296</td>\n","      <td>593</td>\n","      <td>377</td>\n","      <td>589</td>\n","      <td>1210</td>\n","      <td>...</td>\n","      <td>176419</td>\n","      <td>122912</td>\n","      <td>135536</td>\n","      <td>104863</td>\n","      <td>112421</td>\n","      <td>187593</td>\n","      <td>6818</td>\n","      <td>119145</td>\n","      <td>68848</td>\n","      <td>138036</td>\n","    </tr>\n","    <tr>\n","      <th>493</th>\n","      <td>480</td>\n","      <td>260</td>\n","      <td>1196</td>\n","      <td>589</td>\n","      <td>380</td>\n","      <td>356</td>\n","      <td>1210</td>\n","      <td>593</td>\n","      <td>296</td>\n","      <td>32</td>\n","      <td>...</td>\n","      <td>187593</td>\n","      <td>104863</td>\n","      <td>99917</td>\n","      <td>57502</td>\n","      <td>6818</td>\n","      <td>68848</td>\n","      <td>176419</td>\n","      <td>119145</td>\n","      <td>112421</td>\n","      <td>138036</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>606</th>\n","      <td>296</td>\n","      <td>356</td>\n","      <td>593</td>\n","      <td>2959</td>\n","      <td>293</td>\n","      <td>1196</td>\n","      <td>110</td>\n","      <td>608</td>\n","      <td>4878</td>\n","      <td>480</td>\n","      <td>...</td>\n","      <td>156605</td>\n","      <td>30892</td>\n","      <td>104863</td>\n","      <td>68848</td>\n","      <td>26717</td>\n","      <td>6818</td>\n","      <td>148881</td>\n","      <td>153070</td>\n","      <td>176419</td>\n","      <td>57502</td>\n","    </tr>\n","    <tr>\n","      <th>607</th>\n","      <td>480</td>\n","      <td>380</td>\n","      <td>356</td>\n","      <td>593</td>\n","      <td>1196</td>\n","      <td>296</td>\n","      <td>589</td>\n","      <td>2355</td>\n","      <td>260</td>\n","      <td>1210</td>\n","      <td>...</td>\n","      <td>135536</td>\n","      <td>122912</td>\n","      <td>112421</td>\n","      <td>176419</td>\n","      <td>104863</td>\n","      <td>187593</td>\n","      <td>6818</td>\n","      <td>68848</td>\n","      <td>119145</td>\n","      <td>138036</td>\n","    </tr>\n","    <tr>\n","      <th>608</th>\n","      <td>480</td>\n","      <td>296</td>\n","      <td>356</td>\n","      <td>260</td>\n","      <td>1196</td>\n","      <td>110</td>\n","      <td>2959</td>\n","      <td>589</td>\n","      <td>293</td>\n","      <td>593</td>\n","      <td>...</td>\n","      <td>6818</td>\n","      <td>112421</td>\n","      <td>187593</td>\n","      <td>148881</td>\n","      <td>68848</td>\n","      <td>57502</td>\n","      <td>30892</td>\n","      <td>104863</td>\n","      <td>153070</td>\n","      <td>176419</td>\n","    </tr>\n","    <tr>\n","      <th>609</th>\n","      <td>2355</td>\n","      <td>480</td>\n","      <td>380</td>\n","      <td>587</td>\n","      <td>1</td>\n","      <td>185</td>\n","      <td>356</td>\n","      <td>593</td>\n","      <td>296</td>\n","      <td>805</td>\n","      <td>...</td>\n","      <td>167746</td>\n","      <td>104863</td>\n","      <td>158966</td>\n","      <td>6818</td>\n","      <td>135536</td>\n","      <td>122912</td>\n","      <td>187593</td>\n","      <td>68848</td>\n","      <td>119145</td>\n","      <td>138036</td>\n","    </tr>\n","    <tr>\n","      <th>610</th>\n","      <td>5952</td>\n","      <td>2959</td>\n","      <td>356</td>\n","      <td>260</td>\n","      <td>32</td>\n","      <td>296</td>\n","      <td>79132</td>\n","      <td>593</td>\n","      <td>110</td>\n","      <td>2571</td>\n","      <td>...</td>\n","      <td>127172</td>\n","      <td>6232</td>\n","      <td>30892</td>\n","      <td>176419</td>\n","      <td>6669</td>\n","      <td>26717</td>\n","      <td>6770</td>\n","      <td>153070</td>\n","      <td>7061</td>\n","      <td>57502</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>122 rows × 9742 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5aaf5ea-82a9-4a24-9c6b-48c5f711c2e6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a5aaf5ea-82a9-4a24-9c6b-48c5f711c2e6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a5aaf5ea-82a9-4a24-9c6b-48c5f711c2e6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":42}],"source":["%%time\n","result_df = calculator.recommend() # 함수 버전과 마찬가지로, 19초만에 전체 추천 결과 도출!\n","result_df"]},{"cell_type":"markdown","source":["### Result"],"metadata":{"id":"kPlfRoKRvfAr"},"id":"kPlfRoKRvfAr"},{"cell_type":"code","execution_count":null,"id":"8828eb9f","metadata":{"id":"8828eb9f"},"outputs":[],"source":["movies = pd.read_csv(os.path.join(data_path,'movies.csv'))\n","movie_dict = movies.set_index('movieId')['title'].to_dict()\n","\n","result_df_real = result_df.astype(int).applymap(lambda x: movie_dict[x])"]},{"cell_type":"code","source":["result_df_real.to_csv(os.path.join(data_path,'result', 'NRMS_result.csv'))\n","result_df_real"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TJ3ZQnk4ugHM","executionInfo":{"status":"ok","timestamp":1668445566526,"user_tz":-540,"elapsed":2158,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"d533368b-ce8a-4ba1-9482-3105563718f1"},"id":"TJ3ZQnk4ugHM","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 1  \\\n","489                            Forrest Gump (1994)   \n","490                               Inception (2010)   \n","491                              Fight Club (1999)   \n","492                           Jurassic Park (1993)   \n","493                           Jurassic Park (1993)   \n","..                                             ...   \n","606                            Pulp Fiction (1994)   \n","607                           Jurassic Park (1993)   \n","608                           Jurassic Park (1993)   \n","609                           Bug's Life, A (1998)   \n","610  Lord of the Rings: The Two Towers, The (2002)   \n","\n","                                                 2  \\\n","489                            Pulp Fiction (1994)   \n","490  Lord of the Rings: The Two Towers, The (2002)   \n","491                            Pulp Fiction (1994)   \n","492                               True Lies (1994)   \n","493      Star Wars: Episode IV - A New Hope (1977)   \n","..                                             ...   \n","606                            Forrest Gump (1994)   \n","607                               True Lies (1994)   \n","608                            Pulp Fiction (1994)   \n","609                           Jurassic Park (1993)   \n","610                              Fight Club (1999)   \n","\n","                                                     3  \\\n","489                                  Fight Club (1999)   \n","490                                  Fight Club (1999)   \n","491          Star Wars: Episode IV - A New Hope (1977)   \n","492                               Bug's Life, A (1998)   \n","493  Star Wars: Episode V - The Empire Strikes Back...   \n","..                                                 ...   \n","606                   Silence of the Lambs, The (1991)   \n","607                                Forrest Gump (1994)   \n","608                                Forrest Gump (1994)   \n","609                                   True Lies (1994)   \n","610                                Forrest Gump (1994)   \n","\n","                                                 4  \\\n","489  Lord of the Rings: The Two Towers, The (2002)   \n","490      Star Wars: Episode IV - A New Hope (1977)   \n","491                            Forrest Gump (1994)   \n","492                            Forrest Gump (1994)   \n","493              Terminator 2: Judgment Day (1991)   \n","..                                             ...   \n","606                              Fight Club (1999)   \n","607               Silence of the Lambs, The (1991)   \n","608      Star Wars: Episode IV - A New Hope (1977)   \n","609                                   Ghost (1990)   \n","610      Star Wars: Episode IV - A New Hope (1977)   \n","\n","                                                     5  \\\n","489                   Silence of the Lambs, The (1991)   \n","490                                      Avatar (2009)   \n","491  Star Wars: Episode V - The Empire Strikes Back...   \n","492  Star Wars: Episode V - The Empire Strikes Back...   \n","493                                   True Lies (1994)   \n","..                                                 ...   \n","606  Léon: The Professional (a.k.a. The Professiona...   \n","607  Star Wars: Episode V - The Empire Strikes Back...   \n","608  Star Wars: Episode V - The Empire Strikes Back...   \n","609                                   Toy Story (1995)   \n","610          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)   \n","\n","                                                     6  \\\n","489                                Donnie Darko (2001)   \n","490                            Django Unchained (2012)   \n","491      Lord of the Rings: The Two Towers, The (2002)   \n","492                                Pulp Fiction (1994)   \n","493                                Forrest Gump (1994)   \n","..                                                 ...   \n","606  Star Wars: Episode V - The Empire Strikes Back...   \n","607                                Pulp Fiction (1994)   \n","608                                  Braveheart (1995)   \n","609                                    Net, The (1995)   \n","610                                Pulp Fiction (1994)   \n","\n","                                                     7  \\\n","489                                  Braveheart (1995)   \n","490          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)   \n","491          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)   \n","492                   Silence of the Lambs, The (1991)   \n","493  Star Wars: Episode VI - Return of the Jedi (1983)   \n","..                                                 ...   \n","606                                  Braveheart (1995)   \n","607                  Terminator 2: Judgment Day (1991)   \n","608                                  Fight Club (1999)   \n","609                                Forrest Gump (1994)   \n","610                                   Inception (2010)   \n","\n","                                                     8  \\\n","489                             Minority Report (2002)   \n","490  Lord of the Rings: The Return of the King, The...   \n","491                                   Inception (2010)   \n","492                                       Speed (1994)   \n","493                   Silence of the Lambs, The (1991)   \n","..                                                 ...   \n","606                                       Fargo (1996)   \n","607                               Bug's Life, A (1998)   \n","608                  Terminator 2: Judgment Day (1991)   \n","609                   Silence of the Lambs, The (1991)   \n","610                   Silence of the Lambs, The (1991)   \n","\n","                                                     9  \\\n","489  Léon: The Professional (a.k.a. The Professiona...   \n","490                             Minority Report (2002)   \n","491                                  Braveheart (1995)   \n","492                  Terminator 2: Judgment Day (1991)   \n","493                                Pulp Fiction (1994)   \n","..                                                 ...   \n","606                                Donnie Darko (2001)   \n","607          Star Wars: Episode IV - A New Hope (1977)   \n","608  Léon: The Professional (a.k.a. The Professiona...   \n","609                                Pulp Fiction (1994)   \n","610                                  Braveheart (1995)   \n","\n","                                                    10  ...  \\\n","489                                       Fargo (1996)  ...   \n","490                                Forrest Gump (1994)  ...   \n","491  Léon: The Professional (a.k.a. The Professiona...  ...   \n","492  Star Wars: Episode VI - Return of the Jedi (1983)  ...   \n","493          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)  ...   \n","..                                                 ...  ...   \n","606                               Jurassic Park (1993)  ...   \n","607  Star Wars: Episode VI - Return of the Jedi (1983)  ...   \n","608                   Silence of the Lambs, The (1991)  ...   \n","609                             Time to Kill, A (1996)  ...   \n","610                                 Matrix, The (1999)  ...   \n","\n","                                       9734  \\\n","489     A Story of Children and Film (2013)   \n","490                        Born Free (1966)   \n","491                            Ikiru (1952)   \n","492  Avengers: Infinity War - Part I (2018)   \n","493                          What If (2013)   \n","..                                      ...   \n","606      In the Realms of the Unreal (2004)   \n","607  Avengers: Infinity War - Part I (2018)   \n","608                            Frank (2014)   \n","609                          What If (2013)   \n","610                        Born Free (1966)   \n","\n","                                   9735                                9736  \\\n","489                    Born Free (1966)            World of Tomorrow (2015)   \n","490                      Mother! (2017)  In the Realms of the Unreal (2004)   \n","491            World of Tomorrow (2015)                     Begotten (1990)   \n","492                Suicide Squad (2016)                      What If (2013)   \n","493               Upstream Color (2013)       Cat Soup (Nekojiru-so) (2001)   \n","..                                  ...                                 ...   \n","606                      What If (2013)          Brothers Bloom, The (2008)   \n","607                        Frank (2014)                      Mother! (2017)   \n","608                   Deadpool 2 (2018)            World of Tomorrow (2015)   \n","609            Captain Fantastic (2016)  Come and See (Idi i smotri) (1985)   \n","610  In the Realms of the Unreal (2004)                      Mother! (2017)   \n","\n","                                   9737  \\\n","489                 Dark Victory (1939)   \n","490                        Ikiru (1952)   \n","491           My Life Without Me (2003)   \n","492                        Frank (2014)   \n","493  Come and See (Idi i smotri) (1985)   \n","..                                  ...   \n","606                     Begotten (1990)   \n","607                      What If (2013)   \n","608          Brothers Bloom, The (2008)   \n","609                Suicide Squad (2016)   \n","610                        Ikiru (1952)   \n","\n","                                       9738  \\\n","489      In the Realms of the Unreal (2004)   \n","490                         Begotten (1990)   \n","491      In the Realms of the Unreal (2004)   \n","492                       Deadpool 2 (2018)   \n","493              Brothers Bloom, The (2008)   \n","..                                      ...   \n","606      Come and See (Idi i smotri) (1985)   \n","607                       Deadpool 2 (2018)   \n","608           Cat Soup (Nekojiru-so) (2001)   \n","609  Avengers: Infinity War - Part I (2018)   \n","610                         Begotten (1990)   \n","\n","                                   9739                                 9740  \\\n","489                     Begotten (1990)                       Mother! (2017)   \n","490                      Rabbits (2002)            My Life Without Me (2003)   \n","491                 Dark Victory (1939)                       Rabbits (2002)   \n","492  Come and See (Idi i smotri) (1985)  Kingsman: The Secret Service (2015)   \n","493                      Mother! (2017)  Kingsman: The Secret Service (2015)   \n","..                                  ...                                  ...   \n","606            World of Tomorrow (2015)                       Rabbits (2002)   \n","607  Come and See (Idi i smotri) (1985)           Brothers Bloom, The (2008)   \n","608  In the Realms of the Unreal (2004)                       What If (2013)   \n","609                   Deadpool 2 (2018)           Brothers Bloom, The (2008)   \n","610           My Life Without Me (2003)                       Rabbits (2002)   \n","\n","                                    9741                            9742  \\\n","489                       Rabbits (2002)   Cat Soup (Nekojiru-so) (2001)   \n","490                  Dark Victory (1939)   Cat Soup (Nekojiru-so) (2001)   \n","491                       Mother! (2017)   Cat Soup (Nekojiru-so) (2001)   \n","492           Brothers Bloom, The (2008)  The Man from U.N.C.L.E. (2015)   \n","493                         Frank (2014)  The Man from U.N.C.L.E. (2015)   \n","..                                   ...                             ...   \n","606                       Mother! (2017)   Cat Soup (Nekojiru-so) (2001)   \n","607  Kingsman: The Secret Service (2015)  The Man from U.N.C.L.E. (2015)   \n","608                       Rabbits (2002)                  Mother! (2017)   \n","609  Kingsman: The Secret Service (2015)  The Man from U.N.C.L.E. (2015)   \n","610                  Dark Victory (1939)   Cat Soup (Nekojiru-so) (2001)   \n","\n","                                               history  \n","489  [Dracula (Bram Stoker's Dracula) (1992), Lady ...  \n","490  [Melancholia (2011), Simpsons Movie, The (2007...  \n","491  [Forrest Gump (1994), Shrek (2001), Truman Sho...  \n","492  [Independence Day (a.k.a. ID4) (1996), Twelve ...  \n","493  [Princess Bride, The (1987), Mercury Rising (1...  \n","..                                                 ...  \n","606  [Unknown (2006), Whatever Works (2009), Alice ...  \n","607  [True Crime (1996), Gone with the Wind (1939),...  \n","608  [Island, The (2005), It's All Gone Pete Tong (...  \n","609  [Apollo 13 (1995), Pulp Fiction (1994), Dances...  \n","610  [Dog Soldiers (2002), Bittersweet Life, A (Dal...  \n","\n","[122 rows x 9743 columns]"],"text/html":["\n","  <div id=\"df-44ceffa5-161a-49d2-bd2e-4465b363051b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>...</th>\n","      <th>9734</th>\n","      <th>9735</th>\n","      <th>9736</th>\n","      <th>9737</th>\n","      <th>9738</th>\n","      <th>9739</th>\n","      <th>9740</th>\n","      <th>9741</th>\n","      <th>9742</th>\n","      <th>history</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>489</th>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Fight Club (1999)</td>\n","      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Donnie Darko (2001)</td>\n","      <td>Braveheart (1995)</td>\n","      <td>Minority Report (2002)</td>\n","      <td>Léon: The Professional (a.k.a. The Professiona...</td>\n","      <td>Fargo (1996)</td>\n","      <td>...</td>\n","      <td>A Story of Children and Film (2013)</td>\n","      <td>Born Free (1966)</td>\n","      <td>World of Tomorrow (2015)</td>\n","      <td>Dark Victory (1939)</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>Begotten (1990)</td>\n","      <td>Mother! (2017)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>[Dracula (Bram Stoker's Dracula) (1992), Lady ...</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>Inception (2010)</td>\n","      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n","      <td>Fight Club (1999)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Avatar (2009)</td>\n","      <td>Django Unchained (2012)</td>\n","      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n","      <td>Lord of the Rings: The Return of the King, The...</td>\n","      <td>Minority Report (2002)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>...</td>\n","      <td>Born Free (1966)</td>\n","      <td>Mother! (2017)</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>Ikiru (1952)</td>\n","      <td>Begotten (1990)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>My Life Without Me (2003)</td>\n","      <td>Dark Victory (1939)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>[Melancholia (2011), Simpsons Movie, The (2007...</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>Fight Club (1999)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n","      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n","      <td>Inception (2010)</td>\n","      <td>Braveheart (1995)</td>\n","      <td>Léon: The Professional (a.k.a. The Professiona...</td>\n","      <td>...</td>\n","      <td>Ikiru (1952)</td>\n","      <td>World of Tomorrow (2015)</td>\n","      <td>Begotten (1990)</td>\n","      <td>My Life Without Me (2003)</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>Dark Victory (1939)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>Mother! (2017)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>[Forrest Gump (1994), Shrek (2001), Truman Sho...</td>\n","    </tr>\n","    <tr>\n","      <th>492</th>\n","      <td>Jurassic Park (1993)</td>\n","      <td>True Lies (1994)</td>\n","      <td>Bug's Life, A (1998)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Speed (1994)</td>\n","      <td>Terminator 2: Judgment Day (1991)</td>\n","      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n","      <td>...</td>\n","      <td>Avengers: Infinity War - Part I (2018)</td>\n","      <td>Suicide Squad (2016)</td>\n","      <td>What If (2013)</td>\n","      <td>Frank (2014)</td>\n","      <td>Deadpool 2 (2018)</td>\n","      <td>Come and See (Idi i smotri) (1985)</td>\n","      <td>Kingsman: The Secret Service (2015)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>The Man from U.N.C.L.E. (2015)</td>\n","      <td>[Independence Day (a.k.a. ID4) (1996), Twelve ...</td>\n","    </tr>\n","    <tr>\n","      <th>493</th>\n","      <td>Jurassic Park (1993)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Terminator 2: Judgment Day (1991)</td>\n","      <td>True Lies (1994)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n","      <td>...</td>\n","      <td>What If (2013)</td>\n","      <td>Upstream Color (2013)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>Come and See (Idi i smotri) (1985)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>Mother! (2017)</td>\n","      <td>Kingsman: The Secret Service (2015)</td>\n","      <td>Frank (2014)</td>\n","      <td>The Man from U.N.C.L.E. (2015)</td>\n","      <td>[Princess Bride, The (1987), Mercury Rising (1...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>606</th>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Fight Club (1999)</td>\n","      <td>Léon: The Professional (a.k.a. The Professiona...</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Braveheart (1995)</td>\n","      <td>Fargo (1996)</td>\n","      <td>Donnie Darko (2001)</td>\n","      <td>Jurassic Park (1993)</td>\n","      <td>...</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>What If (2013)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>Begotten (1990)</td>\n","      <td>Come and See (Idi i smotri) (1985)</td>\n","      <td>World of Tomorrow (2015)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>Mother! (2017)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>[Unknown (2006), Whatever Works (2009), Alice ...</td>\n","    </tr>\n","    <tr>\n","      <th>607</th>\n","      <td>Jurassic Park (1993)</td>\n","      <td>True Lies (1994)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Terminator 2: Judgment Day (1991)</td>\n","      <td>Bug's Life, A (1998)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n","      <td>...</td>\n","      <td>Avengers: Infinity War - Part I (2018)</td>\n","      <td>Frank (2014)</td>\n","      <td>Mother! (2017)</td>\n","      <td>What If (2013)</td>\n","      <td>Deadpool 2 (2018)</td>\n","      <td>Come and See (Idi i smotri) (1985)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>Kingsman: The Secret Service (2015)</td>\n","      <td>The Man from U.N.C.L.E. (2015)</td>\n","      <td>[True Crime (1996), Gone with the Wind (1939),...</td>\n","    </tr>\n","    <tr>\n","      <th>608</th>\n","      <td>Jurassic Park (1993)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n","      <td>Braveheart (1995)</td>\n","      <td>Fight Club (1999)</td>\n","      <td>Terminator 2: Judgment Day (1991)</td>\n","      <td>Léon: The Professional (a.k.a. The Professiona...</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>...</td>\n","      <td>Frank (2014)</td>\n","      <td>Deadpool 2 (2018)</td>\n","      <td>World of Tomorrow (2015)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>What If (2013)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>Mother! (2017)</td>\n","      <td>[Island, The (2005), It's All Gone Pete Tong (...</td>\n","    </tr>\n","    <tr>\n","      <th>609</th>\n","      <td>Bug's Life, A (1998)</td>\n","      <td>Jurassic Park (1993)</td>\n","      <td>True Lies (1994)</td>\n","      <td>Ghost (1990)</td>\n","      <td>Toy Story (1995)</td>\n","      <td>Net, The (1995)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Time to Kill, A (1996)</td>\n","      <td>...</td>\n","      <td>What If (2013)</td>\n","      <td>Captain Fantastic (2016)</td>\n","      <td>Come and See (Idi i smotri) (1985)</td>\n","      <td>Suicide Squad (2016)</td>\n","      <td>Avengers: Infinity War - Part I (2018)</td>\n","      <td>Deadpool 2 (2018)</td>\n","      <td>Brothers Bloom, The (2008)</td>\n","      <td>Kingsman: The Secret Service (2015)</td>\n","      <td>The Man from U.N.C.L.E. (2015)</td>\n","      <td>[Apollo 13 (1995), Pulp Fiction (1994), Dances...</td>\n","    </tr>\n","    <tr>\n","      <th>610</th>\n","      <td>Lord of the Rings: The Two Towers, The (2002)</td>\n","      <td>Fight Club (1999)</td>\n","      <td>Forrest Gump (1994)</td>\n","      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n","      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n","      <td>Pulp Fiction (1994)</td>\n","      <td>Inception (2010)</td>\n","      <td>Silence of the Lambs, The (1991)</td>\n","      <td>Braveheart (1995)</td>\n","      <td>Matrix, The (1999)</td>\n","      <td>...</td>\n","      <td>Born Free (1966)</td>\n","      <td>In the Realms of the Unreal (2004)</td>\n","      <td>Mother! (2017)</td>\n","      <td>Ikiru (1952)</td>\n","      <td>Begotten (1990)</td>\n","      <td>My Life Without Me (2003)</td>\n","      <td>Rabbits (2002)</td>\n","      <td>Dark Victory (1939)</td>\n","      <td>Cat Soup (Nekojiru-so) (2001)</td>\n","      <td>[Dog Soldiers (2002), Bittersweet Life, A (Dal...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>122 rows × 9743 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44ceffa5-161a-49d2-bd2e-4465b363051b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-44ceffa5-161a-49d2-bd2e-4465b363051b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-44ceffa5-161a-49d2-bd2e-4465b363051b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["result_df[1].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-ryKwQdEkwA","executionInfo":{"status":"ok","timestamp":1668445005266,"user_tz":-540,"elapsed":304,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"af2fc9e4-e642-426a-d929-0f7aa9fd822c"},"id":"6-ryKwQdEkwA","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["480      38\n","296      21\n","79132    16\n","356      11\n","260      10\n","2355     10\n","5952      7\n","2959      6\n","593       2\n","58559     1\n","Name: 1, dtype: int64"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","execution_count":null,"id":"62345e0b","metadata":{"id":"62345e0b"},"outputs":[],"source":["behav_unique = pd.read_csv(behaviors_file,sep='\\t',header=None).drop_duplicates(subset=[0])\n","behav_unique[4] = behav_unique[2].apply(lambda x: x.split(' '))\n","\n","def history_to_movies(history_list):\n","    for i in range(len(history_list)):\n","        history_list[i] = movie_dict[int(history_list[i])]\n","    return history_list\n","\n","behav_unique[4] = behav_unique[4].apply(lambda x: history_to_movies(x))\n","\n","user_history = behav_unique.set_index(0)[4].to_dict()\n","result_df_real['history'] = pd.DataFrame(result_df_real.reset_index()['index'].apply(lambda x: user_history[x])).set_index(result_df_real.index)"]},{"cell_type":"code","execution_count":null,"id":"62a2bad2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62a2bad2","executionInfo":{"status":"ok","timestamp":1668445028440,"user_tz":-540,"elapsed":3,"user":{"displayName":"김그핵","userId":"00855767038371961716"}},"outputId":"6d05581b-fd33-4dbf-b9e8-867f9cf6e2dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Heartbreak Kid, The (2007)', 'Before Sunrise (1995)', 'Hangover, The (2009)', 'Legally Blonde (2001)', 'Harry Potter and the Chamber of Secrets (2002)', 'Harry Potter and the Deathly Hallows: Part 1 (2010)', 'Harry Potter and the Deathly Hallows: Part 2 (2011)', 'Harry Potter and the Half-Blood Prince (2009)', 'Harry Potter and the Order of the Phoenix (2007)', 'Avatar (2009)'] \n","\n"," 1                                      Inception (2010)\n","2         Lord of the Rings: The Two Towers, The (2002)\n","3                                     Fight Club (1999)\n","4             Star Wars: Episode IV - A New Hope (1977)\n","5                                         Avatar (2009)\n","6                               Django Unchained (2012)\n","7             Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n","8     Lord of the Rings: The Return of the King, The...\n","9                                Minority Report (2002)\n","10                                  Forrest Gump (1994)\n","Name: 490, dtype: object\n"]}],"source":["user_nth = np.random.randint(result_df_real.shape[0])\n","history_num = 10\n","rec_num = 10\n","\n","print(result_df_real['history'].iloc[user_nth][-history_num:], '\\n\\n', result_df_real.iloc[user_nth][:rec_num])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":["2e290fe6","fa80a9a4","b63a3b4e","96315ab5","caf8ae18","dc0129f0","d5447178","e38c6e8e","1fb8855e","f3146482","8JRNqUUsvYe-"]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}